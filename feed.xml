<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>94geek.com</title>
    <description>属于大嘴的个人blog</description>
    <link>http://www.94geek.com/</link>
    <atom:link href="http://www.94geek.com/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title> 扩展vagrant磁盘映像</title>
        <description>&lt;p&gt;事情的起因是安装llvm+clang的时候一直不成功。提示说没有磁盘空间了。因为制作vagrant的box的时候，我选择的是自动扩展虚拟磁盘，所以也没有往磁盘真的没有了这个问题上想。还以为编译的时候有些operation没有选对，捣鼓了一阵子。后来执行了一下df才发现vagrant中的虚拟磁盘竟然真的不够用了。这至少告诉我们2个问题：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;vagrant不能自动扩展磁盘；&lt;/li&gt;
  &lt;li&gt;使用vagrant制作的vbox的映像不能继承原来vbox中的设置；      &lt;br /&gt;
那怎么样才能让vagrant中的磁盘映像扩大呢？&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;我的vagrant是架在mac上的，因为穷，所以虚拟机选择了vbox。根据vagrant和vbox的关系，vagrant实际上是接管了在vbox中启动的虚拟机。所以重点还是要扩展在vbox中这个虚拟机映像的磁盘大小。&lt;/p&gt;

&lt;p&gt;起先，还是想到了老办法：原来在用虚拟机的时候记得可以直接在虚拟机中就可以调整磁盘大小的，但是这招在这里不可行，没有找到任何可以设置或者扩展磁盘大小的设置。然后找了一下vbox的文档，发现VBoxManage命令可以更改磁盘印象的大小。&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;先要找到vagrant接管的磁盘映像放在了哪里？执行命令：&lt;code class=&quot;highlighter-rouge&quot;&gt;VBoxManage list hdds&lt;/code&gt;，如下：
&lt;img src=&quot;vbox-hd-list.jpg&quot; alt=&quot;box-list-hd&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;找到了磁盘映像后就可以更改其大小了。这里有两种方法，一种使用磁盘路径作为参数，另外一种是使用如图所示的第一行UUID作为参数。我们使用UUID（因为短），执行命令：
 &lt;code class=&quot;highlighter-rouge&quot;&gt;VBoxManage modifyhd &quot;5f25beda-db03-4801-9522-a56403878304&quot; --resize 15360&lt;/code&gt;
 将磁盘映像调整到15G，结果报错，如下图：
 &lt;img src=&quot;vbox-modify-error.jpg&quot; alt=&quot;box-modify-error&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;报错提示“格式不支持执行的操作”。但是找了半天，都说vmdk格式也可以动态扩展大小，但是不知道为什么VBoxManage不支持（HEI：难道是因为vmdk是vmware标准格式？）。但是VBoxManage命令支持vdi格式(HEI：vdi是vbox私有的格式。)。那就把vmdk格式转换成vdi吧！还是使用VBoxManage命令：
 &lt;code class=&quot;highlighter-rouge&quot;&gt;VBoxManage clonehd /Users/xuhaifeng/VirtualBox\ VMs/vagrant_default_1493825204615_42659/box-disk1.vmdk   /Users/xuhaifeng/VirtualBox\ VMs/vagrant_default_1493825204615_42659/box-disk1.vdi --format VDI&lt;/code&gt;  &lt;br /&gt;
 转换成功，结果如图所示：  &lt;br /&gt;
  &lt;img src=&quot;vbox-disk-cast.png&quot; alt=&quot;box-disk-cast&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;再扩展vdi的大小，还是使用VBoxManage命令，但因为vdi没有被挂载在系统中，所以参数使用路径，如下：
  &lt;img src=&quot;vbox-modify-disk.png&quot; alt=&quot;vbox-modify-disk.png&quot; /&gt;
  扩展成功。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;将原来vagrant接管的虚拟机的vmdk磁盘映像换成刚刚生成的这个vdi的磁盘映像。打开vbox，点击SATA端口的右边将其换掉，如图所示：
 &lt;img src=&quot;vbox-change-disk.png&quot; alt=&quot;box-change-disk&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;启动vagrant，发现报了很多“磁盘空间不够”的错误。如下图：
 &lt;img src=&quot;vagrant-up-error-disksize.png&quot; alt=&quot;vagrant-up-error-disksize&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;登陆系统，发现扩展的空间确实没有被加上去，如下图：
 &lt;img src=&quot;login-vagrant-df.png&quot; alt=&quot;login-vagrant-df&quot; /&gt;&lt;/p&gt;

&lt;p&gt;但是使用fdisk查看一下，发现磁盘的大小确实已经加上了，但是没法使用。如图所示：
 &lt;img src=&quot;fdisk-dev.png&quot; alt=&quot;fdisk-list-dev&quot; /&gt;&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;把新扩展的磁盘大小给加上去。还是使用fdisk命令：
 &lt;code class=&quot;highlighter-rouge&quot;&gt;fdisk /dev/sda&lt;/code&gt;  &lt;br /&gt;
 沿着命令，一路根据提示，输入：
 &lt;code class=&quot;highlighter-rouge&quot;&gt;n &lt;/code&gt;{new partition}&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt; p&lt;/code&gt; {primary partition}&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;3&lt;/code&gt; {partition number}&lt;/p&gt;

    &lt;p&gt;[提示修改大小，默认直接回车]&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;t&lt;/code&gt;{change partition id}&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt; 3&lt;/code&gt; {partition number}&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;8e&lt;/code&gt; {Linux LVM partition}&lt;/p&gt;

    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;w&lt;/code&gt;     &lt;br /&gt;
 如图所示：
  &lt;img src=&quot;fdisk-operator.png&quot; alt=&quot;fdisk-operator&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;再重启vagrant，发现启动的时候还是很多“磁盘空间不足”的报错信息，先忽略它。登入系统后，执行命令：
 &lt;code class=&quot;highlighter-rouge&quot;&gt;fdisk -l /dev/sda&lt;/code&gt;  &lt;br /&gt;
 发现我们加上去的那块磁盘已经又块设备的标识了。如下图：
 &lt;img src=&quot;fdisk-list-dev.png&quot; alt=&quot;fdisk-dev&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;调整卷的大小，使用命令：&lt;code class=&quot;highlighter-rouge&quot;&gt;vgdisplay &lt;/code&gt;先看一下卷的信息，记住 VG Name这个选项的值，然后我们依次使用命令&lt;/p&gt;
    &lt;ol&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;pvcreate /dev/sda3&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;vgextend vg_xvhfeng /dev/sda3&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;lvextend /dev/vg_xvhfeng/lv_root /dev/sda3&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;resize2fs /dev/vg_xvhfeng/lv_root &lt;/code&gt;  &lt;br /&gt;
 将其合并到原来的挂载点中。如下图所示：
 &lt;img src=&quot;mount-disk.png&quot; alt=&quot;mount-disk&quot; /&gt;&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;10.最后，再重启机器，登录后执行df命令，发现磁盘的大小终于被扩展大了。如下图：
&lt;img src=&quot;last-df.png&quot; alt=&quot;last-df&quot; /&gt;&lt;/p&gt;

&lt;p&gt;大功告成。&lt;/p&gt;

</description>
        <pubDate>Sat, 10 Mar 2018 00:00:00 +0800</pubDate>
        <link>http://www.94geek.com/blog/2018/vagrant-disk/</link>
        <guid isPermaLink="true">http://www.94geek.com/blog/2018/vagrant-disk/</guid>
      </item>
    
      <item>
        <title> 全自动无手工干预实现“朋友圈@微信官方头像添加圣诞帽”的解决方案</title>
        <description>&lt;p&gt;首先我声明：我的主要目的是来科普的，请叫我雷锋！&lt;/p&gt;

&lt;p&gt;起因我就不多说了，相信大家都已经被微信朋友圈内“@微信官方，请给我一顶圣诞帽”刷屏了！如图：
&lt;img src=&quot;001.jpeg&quot; alt=&quot;001-c&quot; /&gt;
但结果……..望穿秋水!&lt;/p&gt;

&lt;p&gt;诚然，微信没有实现这个功能。&lt;/p&gt;

&lt;p&gt;本来就知道是恶作剧，我自己也在群里玩。作为技术人员，虽然稍微的想了一下技术方案，但是也没深究。原本这件事情就在嘻嘻哈哈中过去了，但下午看到CSDN技术头条的一篇推送，主要讲述了“朋友圈@微信官方头像添加圣诞帽”是无法实现的，原因……吧啦吧啦吧啦吧啦……..！哎，好歹也是技术社区，怎么说也去知乎啥的发个帖，怎么就引用了娱乐版的“悟空问答”呢？&lt;/p&gt;

&lt;p&gt;虽然微信没这个功能，但并不代表实现不了！在现有的技术环境下，这点需求都实现不了也未免太看不起我们程序员了吧？！哎，难得星期天放个假，还是一个大晴天，我还不闲着非要较个真？这么简单的功能无法实现？我终于知道为啥很多公司的产品经理和程序员之间一直水火不容了。
&lt;img src=&quot;002.jpeg&quot; alt=&quot;xxx-c&quot; /&gt;&lt;/p&gt;

&lt;p&gt;废话不说了，作为一个“好人”，重点还是我们的“科普大业”：怎么样全自动无手工干预的实现“朋友圈@微信官方头像添加圣诞帽”。&lt;/p&gt;

&lt;p&gt;首先，我们先变身为产品，分析需求，如下：
&lt;img src=&quot;006.png&quot; alt=&quot;006-c&quot; /&gt;&lt;/p&gt;

&lt;p&gt;从上图可以看出：“@”是发动者，头像更变是最后的结果。对于程序员来说，需要处理的问题点有几个：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;如何知道用户发起来需要戴圣诞帽的要求；&lt;/li&gt;
  &lt;li&gt;如何识别出用户头像图片中的头部位置及其走向，然后尽可能正确的加上圣诞帽；&lt;/li&gt;
  &lt;li&gt;如何通知微信程序更新最新头像；&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;首先，我们来解决第一个难点： 如何知道用户发起需要戴圣诞帽的要求？    &lt;br /&gt;
用户发一条朋友圈，微信服务器可以完整的得到该朋友圈的内容，所以微信后台可以对其进行内容解析。只要解析到朋友圈内容中带有“@微信官方”并且内容是“加/戴圣诞帽”相关的，就将该条朋友圈看做是用户需要更改头像。在技术上，我们将其视为给微信发送了一个通知，考虑到朋友圈的发送量和改变图片的实时性需求，稍微延迟一点更改头像也没有特别大的关系，所以将处理头像设计成异步模式，最简单的办法就是将通知放入队列，缓存起来。架构图如下：
&lt;img src=&quot;007.png&quot; alt=&quot;007-c&quot; /&gt;&lt;/p&gt;

&lt;p&gt;第二步：解决图片识别问题。   &lt;br /&gt;
鉴于头像图片识别头部的需求，我们不需要太过于精确，所以也没有必要大张旗鼓的使用很多的AI脸部识别技术（PS：当然有更好），目前开源的项目中opencv即可解决识别问题。opencv可以识别出人脸在图片中位置，将其用坐标的形式“框”起来。考虑到有些用户头像的头部有干扰项：比如头部正好是黑色背景、头像中的人是光头……，可以适当的增加一些抗干扰的考虑项；如果要求更高一些，还可以根据眼睛、鼻子、耳朵等头部特征将头部的走向（防止有歪头的）大体计算清楚。这样在得到头部大体大小、坐标、走向的情况下，适当的调整圣诞帽的大小、坐标、走向，合并成一张图片即可；&lt;/p&gt;

&lt;p&gt;最后：通知微信更新最新头像。  &lt;br /&gt;
微信作为一个社交产品，它肯定会考虑加圣诞帽这个需求的时间效应：圣诞帽头像严格意义上说是具有极强时间效应的，所以这个头像存在的时间很短。用户更改头像的初衷并不是出于真心的喜欢，而只是图新鲜，2-5天度过圣诞节后即没用了。所以微信没必要真正的去更改用户的头像，这也就意味着我们根本不需要更改任何数据库的操作，我们直接从原头像的基础上更改，将更改好的图片使用原来头像的文件名推入缓存系统、并且通知微信app更新即可。只要保证在2-5之内头像存在就可以完成任务。当然，如果需要永久保留图片，这就增加数据库标志位字段，然后将加好帽子的图片存入到图片服务器，并且更新缓存服务器的头像图片；&lt;/p&gt;

&lt;p&gt;综上所述，整套架构如下所示：
&lt;img src=&quot;008.png&quot; alt=&quot;008-c&quot; /&gt;&lt;/p&gt;

&lt;p&gt;整个架构图都画出来，但是肯定还有很多的同学会懵逼，竟然不把生成的图片都存储下来？好吧，顺便也科普一下行业内存储图片和处理图片的常用方案。  &lt;br /&gt;
业内处理图片有一个不成文的规定：** 存原图，给效果图。** 展开来说就是：
** 程序在存储用户上传图片的时候尽量不处理或者少处理用户的图片，等到用户获取的时候再由程序根据当时的需求给出不同的图片。 **&lt;/p&gt;

&lt;p&gt;我们最常见的图片操作：加水印、裁剪图片、缩放图片、压缩图片大小……，很多时候我们都在上传的时候解决了。这种方案不是不可以，但是并不好。不好的地方主要有2个：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;浪费存储空间。上传的时候进行处理，也就意味着程序要存储一张原图，还有n张效果图。这就是存储可能被扩大了n倍；&lt;/li&gt;
  &lt;li&gt;扩展难。目前的业务需求图片是200&lt;em&gt;200；过了3个月，业务方需要400&lt;/em&gt;400；又过了2个月，业务方又需要一个300*300的……，纵使我们新需求可以通过更改代码来解决，老图片是不是还要创建一个job来遍历的跑一下才行？&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;所以，图片处理正确的做法应该是：在用户获取图片的时候交给图片的http服务器处理。这种解决方案会在一定程度上增加http服务器的压力，但我们可以使用varlish/squid/cdn等成熟的缓存系统将其做到“尽可能的一次写N次读”。图片只有在第一次被访问的时候需要图片http服务器对其进行处理，以后只要这张图片不过期/不被回收，这张图片一直存在于缓存中，根据反向代理原理，请求在缓存处已经获取文件，故不会对图片http产生任何压力。虽然技术实现难度有所增加，但是却解决上面提到的2个最主要的问题。&lt;/p&gt;

&lt;p&gt;注：以上方案纯属大体方向和个人揣测，实践中还需要再根据具体的业务做具体的调整。
       如有雷同，纯属巧合！&lt;/p&gt;

</description>
        <pubDate>Sat, 23 Dec 2017 00:00:00 +0800</pubDate>
        <link>http://www.94geek.com/blog/2017/wechat-image/</link>
        <guid isPermaLink="true">http://www.94geek.com/blog/2017/wechat-image/</guid>
      </item>
    
      <item>
        <title>情怀初体验--二周写书记</title>
        <description>&lt;p&gt;从下定决心要开始写书到今天，已经二周有余。在这段时间里，除了工作，剩下的时间基本上全部都是和这本书有关：写书、看书、查资料、思考……,几乎每天都是挑灯夜战到半夜1点。谁让我这么矫情，非要为了那份情怀和感情呢？&lt;/p&gt;

&lt;p&gt;拟定目录和大纲的时候一共拟定了4个部分，大概有9-10章。开始我着手的并不是前言、也不是第一章、更不是最后一章或者后记，而是中间的章节：C语言特殊性与解决方案。这一章主要讲解C语言中比较特殊的、具有和别的语言不一样的特性语法、使用注意事项和解决方案，更有很多从坑中爬出来的经验分享。之所以要首先写这一章，主要还是因为以下几点：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;我对C语言比较熟悉，其中几个经常用的特性都比较的清楚，经常会掉进坑里的陷阱也比较熟悉，我想用最快的速度看看在最短的时间内能写多少；&lt;/li&gt;
  &lt;li&gt;计算机语言写成的程序之所以能运行，和操作系统、编译器、连接器等基础设施密切相关，我也想粗略的估计一下经常使用的特性中，需要注意的特性有多少是和基础设施相关的，以好用最短的篇幅来写第一章关于基础知识的内容；&lt;/li&gt;
  &lt;li&gt;这一章在整本书的位置上比较重要，也是基础的章节，后面所有的内容都建立在这一章的基础上。虽然不是C语言的基础知识介绍，但这一章不过关，后面的章节也很难过关；&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;从申请内存开始，到释放内存，然后写stack溢出，再到c语言中的字符串。目前已经花了2周有余，但这一章还没写完。时间已经大大的超出了我的预估。按照我的预计，速度应该在一天写一节。这样，整章基本上也只需要一个星期，最多10天就能搞定。这完全是一种非常乐观的，没有写技术书经验的预估方法导致的（PS：以前写过口水书，类小说。）。和实际差的太远了。虽然一章还没写完，但是写到现在，总结如下：&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;难。虽然我对写技术书的难度在思想上有所准备，但真的开始写了，还是觉得有很多的准备不足。写技术书最大的难点在于：你怎么把你所知道的知识和信息有条理的、清清楚楚的、由易到难的晨晨展现给大家，并且可以让大家相对简单和容易的在短时间接受并且消化，如果知识点有相关的重点，如何向读者表述你所要表述的重点也是一个很重要的技巧，也是相当有难度；&lt;/li&gt;
  &lt;li&gt;多。技术书的工作量明显多于非技术书。IT技术书的工作量多就主要多在了有很多的示例上。示例的工作量简直就是要了人命了。特别是当示例有好几种可能性结果时，需要把这些可能性通过代码和环境的联合作用运行出不同的结果，n份代码和n个环境设置，想想这个工作量……,简直就是让人抓狂；&lt;/li&gt;
  &lt;li&gt;烦。一图胜千言，这句话在代码界中绝对是一顶一的真理。但是因为载体的限制，静态的载体并不能很好的表达出程序运行的整个过程，特别是一些细微的变化，比如指针的指向改变等等。只能把整个过程通过步骤化的方式，将步骤一步一步的静态化处理成图，然后尽可能完整的画出来变化额蛛丝马迹。整个静态化的过程和画图的过程相当的繁琐不说，静态化后很容易丢失一些关键信息。很多时候画了擦，擦了画，画了再擦……周而复始；&lt;/li&gt;
  &lt;li&gt;累。写书真的很累。没有自己的业余时间、没有休息时间、没有双休日，甚至连自己洗澡的时候都会去想下面的知识点要怎么写？这本书是不是还缺些什么特性或者功能没有？我这样写是不是能表达我的意思？是不是有更好的表达方式？然而当你的实际工作量大大超过你的预估的时候，你又想精益求精而准备返工的时候，你就会觉得更累了；&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;情怀和感情，人生中最大的乐趣。所以坚持，坚持下去……&lt;/p&gt;

</description>
        <pubDate>Mon, 11 Sep 2017 00:00:00 +0800</pubDate>
        <link>http://www.94geek.com/blog/2017/c-book-think/</link>
        <guid isPermaLink="true">http://www.94geek.com/blog/2017/c-book-think/</guid>
      </item>
    
      <item>
        <title>为了情怀，我又想写书了</title>
        <description>&lt;p&gt;对于一个写过书，知道写书辛苦、发誓不想再写书的人来说，这几天一直有一个想法在我心里挥之不去：我想写一本书，不是为了钱，也不想为了名，只是想作为一个礼物送给我的那份情怀。&lt;/p&gt;

&lt;p&gt;这本书的主要内容是关于C语言高性能编程的。但是首先，不希望这本书写的有多么的厚；也不希望写一本大而全但全是“Hello world”式的介绍无深度书。我只想把我这7-8年时间对于C语言的项目实践能毫无保留的输出出来。包括C语言的一些基础知识、项目实践、经验总结等等，希望能给更多的人带来便利。&lt;/p&gt;

&lt;p&gt;之所以想出一本这样的书，有几个原因：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;目前市面上关于C语言开发的书，除了讲语法的，就是讲数据结构和算法的。真正用来讲项目实践的，输出项目经验的书少之又少，可以说没有；&lt;/li&gt;
  &lt;li&gt;还有是为了各位我们的小伙伴。我试图让小伙伴们去看书，我还给开了一个详细的C语言项目“打怪升级”书单，但实际收效甚微。总结原因：对于他们来说，看书是很容易，但因为项目经验的欠缺，导致他们无法分辨书中哪些是重点，项目中肯定会用的；哪些是需要知道就行了，项目中很少使用；哪些是压根不需要看的；像这样的情况很多次的出现，几乎每个人都会在这个上卡了很长时间，导致时间花了，但收获很小；&lt;/li&gt;
  &lt;li&gt;这不仅仅是为了大家，也是为了我自己。我们在项目的实施中，新人不断的加入，但是对于C语言的知识、技巧、经验却要重复的对他们进行培训。有时候一个问题要说好几遍，费时费力、投入和产出还不成正比；&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;所以注定了，这本书是一本小众的书。这本书面向的是有一定开发经验的程序员，至少你是一个熟悉C语言的人，曾经用C语言的语法写过程序。如果你还是用过别的语言比如java或者c#那就更好了。因为C语言的坑比较多，掌握曲线比较陡峭，所以一直没有自信心在正式的环境中使用并且部署过C语言程序的人也是这本书的潜在对象。&lt;/p&gt;

&lt;p&gt;干脆，我把这本书暂时就取名为《C语言高性能编程》，我打算整本书分为几个部分：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;操作系统原理与程序结构，主要讲解C语言编程中需要使用到的操作系统的原理，内存的映射，进程的原理等等；&lt;/li&gt;
  &lt;li&gt;C语言。但是不会讲解C语言的语法，使用方法等等。这些应该去看K&amp;amp;R，而是会介绍C语言的几个在项目中特别的注意点，比如内存的申请与释放、野指针问题、stack溢出等等实际的问题和解决方案；&lt;/li&gt;
  &lt;li&gt;实际的项目经验。包括在实际项目中内存的管理，客户端-服务器模式的实现，事件机制，服务器端编程技巧等等。这是整本书的重中之重；&lt;/li&gt;
  &lt;li&gt;程序调试。分为3个部分。首先是gdb的普通调试，然后是valgrand的内存泄露检测，最后是线上环境的bug调试与解决方案；&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;写书是一件苦差事，肯定会影响到公众号的文章。所以为了弥补，我会设定一些样章，我会把这些写好的样章公布在公众号中。希望大家能喜欢。&lt;/p&gt;

</description>
        <pubDate>Mon, 04 Sep 2017 00:00:00 +0800</pubDate>
        <link>http://www.94geek.com/blog/2017/c-book/</link>
        <guid isPermaLink="true">http://www.94geek.com/blog/2017/c-book/</guid>
      </item>
    
      <item>
        <title>mysql连接池不能回避的wait timeout问题</title>
        <description>&lt;p&gt;感谢我们的木木同学给了我写这篇文章的灵感和机会。&lt;/p&gt;

&lt;h2 id=&quot;起因&quot;&gt;起因&lt;/h2&gt;
&lt;p&gt;我们的项目组一直在使用albianj作为开发框架在开发应用。使用至今倒也是没有出现很大的问题，但最近加过监控的接口基本上都会在使用一段时间后，突然之间执行数据库操作变得很慢。虽然会变慢，但持续的时间比较短，一般1分钟左右，然后会自动恢复正常。但是过了一段时间，这个现象又会出现，周而复始。从监控看，发生的时间点并无规律，有的时候一天发生3次，有的也会有4-5次。虽然从规律上并无法去查找，那就只能从别的地方想办法：增加一些详细的日志，从日志上看一下问题所在。&lt;br /&gt;
详细日志版本刚刚上去，立刻就发生问题了。如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;3.png&quot; alt=&quot;problem&quot; /&gt;&lt;/p&gt;

&lt;p&gt;看一下左下角的曲线图，突然飙高，然后在1500ms的高位不下来。经过查找日志（PS：sorry。写文章的时候日志已经被自动清除，没法截图了），发现程序卡在了getConnection这个方法上，并且卡住的时间从60s开始越来越长。&lt;/p&gt;

&lt;h2 id=&quot;分析&quot;&gt;分析&lt;/h2&gt;
&lt;p&gt;getConnection是一个synchronized方法，主要是从连接池中获取数据连接！卡住时间越来越长这个现象倒是很简单就可以解释：因为getConnection是synchronized的，所以所有的线程到getConnection的时候全部等待，等待的时间越长当然时间越长了。关键在于为啥卡住呢？&lt;br /&gt;
当时有两种可能的想法：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;连接池设置的太小，连接在使用的时候来不及被返还，导致了积压；&lt;/li&gt;
  &lt;li&gt;连接池的设置有问题，返回的连接有问题。如果是这个问题，那可能会比较难查；&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;第一种情况显然不会存在，查看albianj的数据库配置，所有的配置对于连接池的设置MinSize为5，MaxSize为20.对于我们的应用来说肯定是够用的。那么就是第二个问题了。&lt;/p&gt;

&lt;p&gt;首先检查连接复用问题，对于数据库的连接字符串，我已经增加了重连的设置：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;autoReconnect=true&amp;amp;failOverReadOnly=false&amp;amp;zeroDateTimeBehavior=convertToNull&amp;amp;maxReconnect=3&amp;amp;autoReconnectForPools=true
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;显然autoReconnect没有起作用。查了一下mysql的客户端说明，后背发凉。基本上的大意翻译过来是这样的&lt;strong&gt;“即使在创建Mysql时url中加入了autoReconnect=true参数,一但这个连接两次访问数据库的时间超出了服务器端wait_timeout的时间限制,还是会CommunicationsException: The last packet successfully received from the server was xxx milliseconds ago. ”&lt;/strong&gt;。赶快去查一下日志，发现并没有报这个错误。那应该不是这个问题。&lt;/p&gt;

&lt;p&gt;既然不能重连接，那么最大的可能就是&lt;strong&gt;“返回的连接本来可能就是有问题的，但是程序确认为没有问题”&lt;/strong&gt;。发生这种问题的最大可能应该就是：&lt;strong&gt;数据库连接池的连接过期时间大于mysql的wait_timeut设置&lt;/strong&gt;。查了一下代码，发现我们默认的数据库连接池连接过期时间是300-30=270s。再去问一下DBA木木，让他查一下线上的数据库wait_timeout设置，被告知是180.瞬间懵逼：连接池的生命周期时间远远参与真实的连接生命周期时间。&lt;/p&gt;

&lt;p&gt;原因是找到了，但是这个问题其实前一段时间已经发现并且已经改过了。因为去年我记得很清楚：又一次我们的probactor（job调度系统）报了上文提到的CommunicationsException异常，后来找到了问题就是因为程序对于连接池中连接的alivetime长于数据库的wait timeout设置，随后我千叮咛万嘱咐这个设置必须要小心小心再小心，但这次还是中招了。因为连接池和网络的问题，我们有同事直接放弃连接池，改用每次连接数据库。放弃连接池确实能解决连接不会出现问题，但是侧面也导致了wait_timeout被设置的过小了。但其实只要把连接池中连接的过期时间设置的比wait_timeout小一些就完全可以了。&lt;/p&gt;

&lt;p&gt;经过更改后的程序终于恢复了正常，看一下更改后的效果：&lt;br /&gt;
&lt;img src=&quot;2.png&quot; alt=&quot;ok&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;科普&quot;&gt;科普&lt;/h2&gt;
&lt;p&gt;何为wait_timeout？&lt;br /&gt;
wait_timeout是mysql的一个设置，主要是用来断开不使用的数据库连接。当连接空闲的时间达到wait_timeout设置的最大值时，mysql会主动切断这个连接，以供别的客户端连接数据库。这个值一般是28800，也就是8小时。在mysql中可以通过：
&lt;code class=&quot;highlighter-rouge&quot;&gt;show variables like “%timeout%”;&lt;/code&gt;
获取。&lt;br /&gt;
另外，当数据库主动切断连接的时候，java的mysql客户端并不知道这个连接已经被切断，所以程序并不知道其已经无效了，然后加上mysql的客户端不支持ReConnect，双重的问题叠加在一起就导致了连接池返回无效连接的可能。这是一个比较扯淡也是一个比较难以发现的问题，但是它确确实实的存在了。大家一定要多加注意。&lt;/p&gt;

&lt;p&gt;这个值可以根据自己网络的环境和业务的并发性来调整。&lt;/p&gt;
</description>
        <pubDate>Tue, 22 Aug 2017 00:00:00 +0800</pubDate>
        <link>http://www.94geek.com/blog/2017/mysql-wait-timeout/</link>
        <guid isPermaLink="true">http://www.94geek.com/blog/2017/mysql-wait-timeout/</guid>
      </item>
    
      <item>
        <title>台湾技术交流见闻与感想</title>
        <description>&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;

&lt;p&gt;这几天有幸因为台湾主办方和国内著名出品人史海峰的邀请前去台湾进行了技术交流。这次技术交流一方面让台湾那边了解目前国内互联网行业的一些技术和状态，另外一方面也让我们可以更加了解台湾互联网技术方面的情况。&lt;/p&gt;

&lt;p&gt;毕竟是来交流技术的，所以先说一下我们这次两岸技术交流的情况和我对于这次交流的一些心得和体会。&lt;/p&gt;

&lt;h2 id=&quot;技术交流&quot;&gt;技术交流&lt;/h2&gt;

&lt;p&gt;从技术交流的现场来说，台湾的技术交流场面很火爆。基本上座无虚席，还有没有抢到主会场的只能看直播。但架构场相对就没有那么理想了，原因后面会讲到。火爆的会场如图：
&lt;img src=&quot;7.jpeg&quot; alt=&quot;交流现场&quot; /&gt;
台湾目前的互联网技术水平和国内相比，说句实话：相差的有点远。在台湾，大多数的工程师都是FullStack类型，并不像是国内的互联网行业：不想当架构师的程序员不是好程序员。而且台湾的技术工作分工也没有国内细分的详细。在台湾最受欢迎的是前端，并不是架构师等后端职位。我们在架构场开讲的时候做过几个调查：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;在座的有多少个是写后端的？举手的只有3个左右，当然我们说的是纯后端。&lt;/li&gt;
  &lt;li&gt;这里有多少个title或者日常工作是架构师？ 举手的只有1个。追问：用什么语言？答：python。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;而我在交流的时候也问了几个问题，有技术也有业务：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;看网络小说的有多少？大概有20个人左右，人还是挺多的；&lt;/li&gt;
  &lt;li&gt;知道阅文集团/起点/腾讯文学任何一家的？前面举手的大概都知道。这里有个情况，其实我们的一些作品有在台湾播放电视剧，我注意一下台湾的电视台，我至少看见有3部：花千骨，芈月传，甄嬛传在播出。后来打听了一下，甄嬛传几乎被当成了“当时内地的还珠格格那种热度”在台湾播出。对我们来说，这个市场还是很大的；&lt;/li&gt;
  &lt;li&gt;技术上就又不一样了，比如讲分库分表和数据路由的时候，基本上下面是一片懵逼。在讲到后面调度系统的时候，quartz这个东西只有3个人听过，cron表达式也只有5个人听过；我调查了一下，写过定时任务的一个都没有。我就有点纳闷：他们的网站难道没有定时任务需要处理吗？&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;我们几个讲师其实也挺费劲，会前讨论应该怎么样让他们听的懂我们在讲什么？会后我们还是在讨论为什么出现这个情况，后来我们总结了一下，主要归结如下：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;用户。台湾一共2200w人，内地15亿。没有用户量爆发式增长，所以很多问题都碰不到，他们也就不用解决，像我们陈老师演讲的时候说，我们一开始规划下来需要26个数据库，马上下面的留言板上说，26个数据库，太庞大了，囧；&lt;/li&gt;
  &lt;li&gt;必要性。按照目前的机器硬件配置，在没有爆发式增长的情况下，没有架构也好，不要后端也罢，只要会写代码，拉几个开源的就可以上了。可能缓存都用不了，不要说什么读写分离、分库分表这些东西了；&lt;/li&gt;
  &lt;li&gt;国外的冲击。米国的互联网台湾都可以用，想fb，google这些都可以，畅通无阻，那么社交他们也主要用line，所以台湾根本没有自主的互联网企业和软件。在“5分钟演讲”环节，有个台湾本地哥们提出来一个问题：台湾有名的软件公司有哪个？只回答了一个“趋势科技”就没了，互联网技术公司一个都没有；&lt;/li&gt;
  &lt;li&gt;重视前端。因为碰不到用户量和数据量的关系，又因为大部分东西都是用国外的，所以剩下的能做的就没几样了。你不做前端做什么呢？&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;台湾的互联网技术除了这些之外，还有几个特点：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;脚本语言占大多数。我们在场内看到的最多的开发者是用nodejs，再下面就是go之类的新兴语言，也有python之类的脚本。但是国内很多互联网公司喜欢用的java，c/cxx这些语言在这里已经绝迹；&lt;/li&gt;
  &lt;li&gt;拼凑。几乎所有的开发都是用了开源的组件拼拼凑凑起来的。印象比较深的是一个哥们讲微服务，然而他并没有去讲微服务怎么怎么实现，而是讲了用了什么什么框架，什么什么组件，在什么什么上，搭起来了。就是那种纯粹的堆积木。我不知道如果这要是放在内地，会不会出去了要挨揍；&lt;/li&gt;
  &lt;li&gt;追新。什么新用什么，不会管这个新的东西到底怎么样，是不是适合，这些好像他们不太会去考虑。上面说到的开源使用，有一些就是觉得为了使用而使用，为了体现我们这个东西是多么的新潮，使用了多么cool的新技术就去使用了，而不会去规划和思考底层的问题；&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这次的技术交流，对于我们内地来的讲师来说，最感同身受的是：&lt;strong&gt;我们的互联网技术确实已经起来了。说句大话，对手可能只有一个了，剩下的都是渣渣！&lt;/strong&gt;&lt;br /&gt;
对于台湾技术来说，要想真的提高，必须也要从商业模式和底层技术开始寻找出路，有了良好的商业模式，有用户开始喜欢使用你们的产品了，自然而然的对一些后端的技术会有更高的要求。技术自然就会跟着起来了。但就从现阶段来看，目前的台湾就像我们在10年前、20年前看硅谷一样：一切都是那么的新奇、牛逼闪闪而高高在上。&lt;/p&gt;

&lt;h2 id=&quot;台大&quot;&gt;台大&lt;/h2&gt;

&lt;p&gt;说完演讲正事，再说一下这次的举办场地。主办发选择了台湾大学的社科院作为举办场地让我受宠若惊。第一次站在学术氛围浓厚的大学进行演讲，难免还是有点紧张。演讲之余我们几个大陆来台的讲师也在台大转了一下，我有几个体会：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;台大的校门很低调，低调到如果是在路上走，你可能会根本没在意它的存在。
&lt;img src=&quot;2.jpeg&quot; alt=&quot;台大校门&quot; /&gt;
只有走进了看，才能看清楚上面的字。
&lt;img src=&quot;1.jpeg&quot; alt=&quot;台大校门详细&quot; /&gt;
怎么说台大也是出了很多名人的地方，不管是政治届、娱乐圈、工商界、科技界还是学术界，名人都是大把大把的不缺。但是这个校门真的太低调；&lt;/li&gt;
  &lt;li&gt;台大内部给我的感觉就像是一个缩小版的东京大学，不管是建筑风格、院系设置还是人文情怀，几乎都是照搬东大而来（PS：后来认识了一个导游哥们，他说台大以前叫帝国大学，是在日本占据台湾的时候兴建的，后来国军入台后才改名台大。回去一查，果然前几任校长都是日本人）。
&lt;img src=&quot;3.jpeg&quot; alt=&quot;图书馆&quot; /&gt;
&lt;img src=&quot;4.jpeg&quot; alt=&quot;台大校门详细&quot; /&gt;
这是国内的电子信息工程系，这里的名字叫什么来着，忘记了，反正挺长而且挺繁琐。
&lt;img src=&quot;5.jpeg&quot; alt=&quot;电子信息工程系&quot; /&gt;
没记错的话应该是日本留学生或者是日本留学生留下的一个雕塑作品（PS：这里可能记不太清了。这个雕塑还有一个是草地上的建筑模型，这两个不知道哪个的作者是日本人了。有点健忘，不好意思）
&lt;img src=&quot;6.jpeg&quot; alt=&quot;地标：阅读的小女孩&quot; /&gt;
这个是台大的图腾。一个钟。上面写着：一天其实只有21个小时，另外3个小时是用来……
&lt;img src=&quot;8.jpeg&quot; alt=&quot;台大的图腾&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;彩蛋&quot;&gt;彩蛋&lt;/h2&gt;
&lt;p&gt;程序员就是程序员，会后主办方安排我们去领略了一下台湾当地的风俗人情。其中有个放天灯的环节，一般人在天灯上无非都留下些什么“身体健康”，“全家幸福”，“xxx爱你一辈子”这种的话。只有我们程序员的天灯是那么的与众不同：&lt;/p&gt;

&lt;p&gt;hello world。其实这个不是我写的，虽然拍的是我。我写的是 root#:rm -rf /
&lt;img src=&quot;9.jpeg&quot; alt=&quot;hello world&quot; /&gt;&lt;/p&gt;

&lt;p&gt;来自前端兄弟的javascript是世界上最好的语言。其实我是想用c反驳的，但是家伙狡猾狡猾的，全写满了，不给机会！
&lt;img src=&quot;10.jpeg&quot; alt=&quot;javascript是世界上最好的语言&quot; /&gt;&lt;/p&gt;

&lt;p&gt;58沈大师的“当场面试”–快排算法。可惜最后“一致不怀好意的”判定有两个问题：第一个，函数没写全，编译不通过；第二个，递归漏写了退出条件，CPU100%。
&lt;img src=&quot;11.jpeg&quot; alt=&quot;快排&quot; /&gt;&lt;/p&gt;

&lt;p&gt;主办方和我们架构场的讲师们
&lt;img src=&quot;12.jpeg&quot; alt=&quot;我们架构场的讲师&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这是我们大陆的所有讲师在101大楼，101大楼发生了很多故事，但是只能发这一张了。大陆讲师就缺钟恒了，家伙跑出去单独行动了，目的比较可疑。
&lt;img src=&quot;13.jpeg&quot; alt=&quot;我们讲师&quot; /&gt;&lt;/p&gt;

</description>
        <pubDate>Sun, 13 Aug 2017 00:00:00 +0800</pubDate>
        <link>http://www.94geek.com/blog/2017/taiwan-ithome/</link>
        <guid isPermaLink="true">http://www.94geek.com/blog/2017/taiwan-ithome/</guid>
      </item>
    
      <item>
        <title>网络read函数未判返回0导致CPU 100%</title>
        <description>&lt;p&gt;我们的“运维小帅哥”又来烦我们了！没事就在群里给我们post了一张图，如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;1.png&quot; alt=&quot;show&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;新上线的lest系统的CPU使用率很高！&lt;/strong&gt;。一看才17%，不是正常吗？小帅哥爆发了，不对。这是多核，被平均的消耗！实际上两个核已经被吃满了！&lt;/p&gt;
&lt;h2 id=&quot;检查服务器&quot;&gt;检查服务器&lt;/h2&gt;

&lt;p&gt;使用top命令看了一下，如下图：&lt;br /&gt;
&lt;img src=&quot;2.png&quot; alt=&quot;top&quot; /&gt;&lt;/p&gt;

&lt;p&gt;从上图可以看到，cpu的load负载一直在2左右，也就是说其中的2个核已经被占满。再看下面，是被2个cpu沾满的！看一下CPU的情况，其中系统负载和用户负载几乎是一样的，一个是9.1%，另外一个是7.4%。这就是说可能的问题出在用户态，用户态因为需要调用系统调用，把系统的负载带起来了！那么造成这种疯狂消耗CPU的原因基本上99%都是在循环中干了什么傻事，导致循环出不来，疯狂的消耗CPU！&lt;/p&gt;

&lt;p&gt;因为是CPU负载比较高，不是什么内存泄露之类的问题，所以没办法通过core来精确的debug。我们只能靠gcore通过人工的方式强行抓取进程的runtimes碰碰运气，看看是不是能看出来一点蛛丝马迹。&lt;/p&gt;

&lt;h2 id=&quot;debug看runtimes&quot;&gt;debug，看runtimes&lt;/h2&gt;

&lt;p&gt;首先，加入core后得到的结果如下图：&lt;br /&gt;
&lt;img src=&quot;3.png&quot; alt=&quot;gdb core&quot; /&gt;&lt;/p&gt;

&lt;p&gt;和上面top对应的是两个线程：25128和24130。因为问题就是出在它们身上，我们进入这2个线程看一下它们到底在做什么，首先看一下线程的编号，如下图：&lt;br /&gt;
&lt;img src=&quot;4.png&quot; alt=&quot;th1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;当前线程就是28这个，那就直接来吧，运行bt，如下图：&lt;br /&gt;
&lt;img src=&quot;7.png&quot; alt=&quot;bt&quot; /&gt;
和前面的几次一样都是啥都没有的？，还是使用%rbp来看一下吧，如下图：&lt;br /&gt;
&lt;img src=&quot;5.png&quot; alt=&quot;rbp&quot; /&gt;
这是在读取数据，这个也是正常的调用，但是它是一个“可能的循环”。看上去没发现什么有价值的东西。那么进入到30这个线程看看，如下图：&lt;br /&gt;
&lt;img src=&quot;6.png&quot; alt=&quot;th30&quot; /&gt;
这个线程还是在stack的顶端，没在运行。也就是说可能在我们抓取core的一瞬间，此线程正好因为cpu的时间片被让出，啥都没在干。&lt;/p&gt;

&lt;p&gt;到目前为止，通过gdb我们没有发现很多有价值的东西，除了那个疑似的可能性循环，但是就这点信息不能确定就是它的问题。也就是说从用户态入手，我们看不太到我们想要的信息，那我们看看系统调用的。&lt;/p&gt;

&lt;h2 id=&quot;再次求证&quot;&gt;再次求证&lt;/h2&gt;

&lt;p&gt;使用strace将所有线程的系统调用全部抓取出来，因为线程太多了，容易形成干扰，所以我们使用grep将怀疑有问题的25128线程给抓取出来，如下图：&lt;br /&gt;
&lt;img src=&quot;8.png&quot; alt=&quot;strace&quot; /&gt;&lt;/p&gt;

&lt;p&gt;为了明确问题，我已经用红色的标记给标出来了！我们发现在整个的系统调用中，一直在循环的调用read，epoll_ctl，这样的函数。再仔细看一下，每次read都欲获取29长度的值（29是header的buffer长度），但是实际得到的长度是0，表示没有数据。也就是说，我们试图每次都获取数据，但每次都没有获取到数据，事件机制又把这个fd给重新加到了epoll中监听了。回忆上面core在gdb中的表现，28线程一直在read数据。那么问题就在这里了。因为没有给read进行合理的返回值过滤，导致错误的返回值也被按照正确对待，重新加入epoll监听，有因为epoll是一个无线循环，而每次又都触发read，所以轻轻松松的就干掉了CPU。&lt;/p&gt;

&lt;h2 id=&quot;检查代码&quot;&gt;检查代码&lt;/h2&gt;

&lt;p&gt;回过头来检查代码，read的网络事件都是被组织在spx（是我们的一个c开发组件）中的，查看nio，如下图：
&lt;img src=&quot;9.png&quot; alt=&quot;code&quot; /&gt;
确实没有对read的真实获取长度（就是代码中的&amp;amp;len）为0的情况进行判断，所以发生了这个问题。解决办法也简单，将这个判断加上就可以了。，如下图：&lt;br /&gt;
&lt;img src=&quot;10.png&quot; alt=&quot;code&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;连带问题及其解决&quot;&gt;连带问题及其解决&lt;/h2&gt;

&lt;p&gt;解决问题，心里美滋滋的。上uat，观察一会儿！突然，我同事和我说，为什么fixed后的版本上传数据失败率有点高啊？多高？50%以上！我x。马上拿一个日志下来看看，如图：
&lt;img src=&quot;11.png&quot; alt=&quot;log&quot; /&gt;&lt;/p&gt;

&lt;p&gt;哎呀，很多重试的链接都被强行关闭了。仔细看一下上面的代码，也就是说当spx_read_to_msg_once的err为EAGIN的时候，len也是0，但是代码却先判断了0==len，所以就“变向”过滤掉了err == EAGIN的情况。但是err == EAGIN的情况是正常的，应该被再次加入epoll重试，所以调整一下代码，将0 == len的判断移到判断err的后面，先判断err，就不会出现这种情况下了，代码更改如下：&lt;br /&gt;
&lt;img src=&quot;12.png&quot; alt=&quot;code&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这样世界终于清静了！上uat，果然畅通，稳定后再上到online，看一下这个cpu的曲线：
&lt;img src=&quot;13.png&quot; alt=&quot;cpu&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在这个40°c的天气终于透心凉了！&lt;/p&gt;

&lt;h2 id=&quot;问题引申&quot;&gt;问题引申&lt;/h2&gt;

&lt;p&gt;那么还有问题来了，为什么write数据的时候，如果返回值为0不需要判断呢？&lt;br /&gt;
其实这个和tcpip的api有关。当read的时候，如果返回值是0，就表示对端已经关闭，所以后续没有数据读取了。write的时候，write的是本地的网络缓冲区，也就是滑动窗口，如果你的额client是一个慢client，读取你滑动窗口的数据特别慢，就会导致你write的时候数据写不进去，但是数据写不进去并不是一个错误，只是要你等client读取过后再试一次而已。所以write返回0加入epoll来使用事件触发是正确的。&lt;/p&gt;
</description>
        <pubDate>Tue, 08 Aug 2017 00:00:00 +0800</pubDate>
        <link>http://www.94geek.com/blog/2017/read-rc-0/</link>
        <guid isPermaLink="true">http://www.94geek.com/blog/2017/read-rc-0/</guid>
      </item>
    
      <item>
        <title>解决锁抢占问题--随机式获取抢占锁</title>
        <description>&lt;h2 id=&quot;背景&quot;&gt;背景&lt;/h2&gt;
&lt;p&gt;我们原本的调度系统是由quartz为基准DIY的系统，但因为quartz的很多问题，特别是可扩展设计是在太差、自定义功能太麻烦，我们不得不自行设计了一个调度系统，内部称为：probactr。probactr分为下面几个节点：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;monitor：监视器，主要负责监视Executor的状态和Executor执行的job状态，如发现Executor出现down机或者job出现问题，会对其进行清理。此节点为可平行扩展集群；&lt;/li&gt;
  &lt;li&gt;Executor：运行器，主要负责从数据库中获取欲执行的job，然后执行job。此节点为可平行扩展集群；&lt;/li&gt;
  &lt;li&gt;LockerServer：分布式锁服务器，为probactr提供一致性功能。目前使用redis替代，有计划将其替换成我们自主研发的lax605；&lt;/li&gt;
  &lt;li&gt;ManagerSite：后台管理系统，可以在这里对job进行添加、删除、暂停等等的管理，也可以查看job的执行状态；&lt;/li&gt;
  &lt;li&gt;database：数据库，所有的job数据全部存入数据库；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;系统结构图如下：
&lt;img src=&quot;1.png&quot; alt=&quot;probactr&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;问题表现&quot;&gt;问题表现&lt;/h2&gt;
&lt;p&gt;probactr在开发环境中没有任何的问题，运行一切正常。上到test环境运行一段时间后发现有几个问题：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;可并行job（job分为可并行和不可并行2种）的统计状态不对；&lt;/li&gt;
  &lt;li&gt;打开邮箱发现报警邮箱已经被塞爆，据报警信息可知：更新job的状态和统计信息失败，并且基本上1s内可以产生3-4封同样的mail；&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;分析问题&quot;&gt;分析问题&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;首先，分析一下报警的mail，除了知道是更新状态和统计信息失败以外，还发现一个问题：所有报警的job都是可并行的job，且同一个trigger触发了很多个job；&lt;/li&gt;
  &lt;li&gt;接着，看一下job的Executor机器监控，发现CPU很高，有一些核能飙到100%；&lt;/li&gt;
  &lt;li&gt;然后，再查看一下数据库，发现被报警的trigger同时运行的job数特别大（我们当时没有对同一个trigger可以触发的job数进行限制）；&lt;/li&gt;
  &lt;li&gt;再查看一下别的job，发现一些不可并行的trigger并没有被触发，都被积压了；&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;到这里问题已经很清楚了，出问题的应该是可并行job导致的。首先想到的是可能是&lt;strong&gt;我们没有限制可并行的job数&lt;/strong&gt;，导致了可并行job并发特别厉害，我们设置阀值，应该就没有问题了。&lt;/p&gt;

&lt;p&gt;我们增加了这个功能，并且上test。发现确实问题减轻了很多，但是并不能完全杜绝。还是会有同样的报警mail出现，只是数量上少了很多。这可以证明问题只是得到了缓解，而并没有彻底解决。所以我们再去找原因。更新job状态的代码只有2处：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;job在启动的时候：trigger被scher扫描到，并且scher认为trigger满足被触发的条件。probactr会执行以下路径的代码：获取job的locker-&amp;gt;启动job-&amp;gt;更新job的状态-&amp;gt;释放locker；&lt;/li&gt;
  &lt;li&gt;job执行完毕的时候：job执行完毕后，也会执行上面同样的逻辑；&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;但是问题出现的位置应该不会是在job启动时，因为我们对于job的启动进行了weak化，job起不来没关系，下次scher扫描到再起来即可。但是在job结束时却是要强制性的，因为这一步job已经执行完成，必须要更新job的状态和信息，所以必须要成功，否则整个probactr的状态就会出问题。&lt;/p&gt;

&lt;p&gt;找到问题就好办了。&lt;/p&gt;

&lt;p&gt;再排查下去，更新数据库应该没有问题，因为就一条特别简单的sql语句，唯一能出现问题的地方应该是获取锁了。在job执行完成后，我们需要同时更新job和trigger的状态，所以必须要先获取locker。那么如果可并行job在同一时间结束，而且在同一时间去获取locker，确实可能会出现获取不到锁，然后相当于一直阻塞的状态。我们把并发job的数量改小，相当于同一时间获取的locker的请求量变小了，但是不保证一定没有，所以看起来问题减轻了。但这一步获取locker必须成功，所以保不齐肯定会磕到门牙，问题还是存在。那么为什么有时候CPU会飙升呢？因为我们必须要获取锁，所以一直在loop这个获取锁的功能，CPU当然撑不住。&lt;/p&gt;

&lt;h2 id=&quot;解决办法&quot;&gt;解决办法&lt;/h2&gt;
&lt;p&gt;首先想到的解决办法是：能不能放弃最后一步获取locker。但是很遗憾，经过一遍代码回溯发现行不通。不要locker的话还是存在数据不一致问题，job的属性状态和job的统计信息会不对；  &lt;br /&gt;
第二：调整获取锁的算法，使用sleep的方式来进行，比如sleep(100ms),这个方法在一定程度上是可以的，但还是会有问题，如果两个job同时抢占locker，如果sleep的时间一样，除了cpu的切换以外，还有一定的概率会第二次，第三次同时被唤醒；  &lt;br /&gt;
最后：我们使用了随机值的算法，在一定的范围内，根据我们的算法生成一个值，然后sleep这个随机值。这样可以巧妙的规避掉同时获取locker失败的问题。&lt;/p&gt;
</description>
        <pubDate>Tue, 18 Jul 2017 00:00:00 +0800</pubDate>
        <link>http://www.94geek.com/blog/2017/staircase-locker/</link>
        <guid isPermaLink="true">http://www.94geek.com/blog/2017/staircase-locker/</guid>
      </item>
    
      <item>
        <title>PHP&quot;伪司机&quot;调试PHP CORE</title>
        <description>&lt;p&gt;这次不是装逼，是真的帮忙找问题。对于php，一脸懵逼啊！因为就从来没写过，根本不懂php！&lt;/p&gt;

&lt;h2 id=&quot;发车&quot;&gt;发车&lt;/h2&gt;
&lt;p&gt;正常发布日，因为dfs的php客户端需要增加api而更新，就是在更新的过程中发生了问题，具体的表象为：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;程序无响应，访问web网页没有显示，直接报无响应；&lt;/li&gt;
  &lt;li&gt;CPU100%，服务器的CPU一直100%，非常“稳定”；&lt;/li&gt;
  &lt;li&gt;磁盘100%，磁盘监控显示也是100%，但是我们没有任何的磁盘操作啊，除了读php的文件，懵逼ing；&lt;/li&gt;
  &lt;li&gt;机器非常慢，执行命令非常慢，慢到无法忍受，甚至打命令都很一个字母一个字母的延迟；&lt;br /&gt;
更加诡异的是虽然我们确实更新了php插件，但是这个插件属于提前更新，业务代码并没有使用到这个插件新的api，所以首先排除是因为新插件新增功能的bug导致的，而老的api我们已经用了一年多了，也不会出现问题。那么只有更新的方式不对了？但是这个更新的方式也用了2年多了，以前一直没问题，怎么就今天出现问题了呢？幸好我们打开了core，php进程在crash的时候生成了core，我们可以用gdb调试一下。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;开车&quot;&gt;开车&lt;/h2&gt;
&lt;p&gt;还是老路子，首先压入core文件，如下图：
&lt;img src=&quot;1.png&quot; alt=&quot;php core&quot; /&gt;  &lt;br /&gt;
从这个图中我们可以得到2个信息，红色的已经标注：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;首先core是由php-fpm引起的，问过同事才知道这个是一个php提供web的组件；&lt;/li&gt;
  &lt;li&gt;生成core是因为信号11，也就是段错误。引起这个问题一般的问题就是内存错误，比如内存没有释放，使用了野指针，或者是溢出等等；&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;导入php的可执行文件，执行bt查看一下当前的stack情况，如下图：&lt;br /&gt;
&lt;img src=&quot;2.png&quot; alt=&quot;php bt&quot; /&gt;  &lt;br /&gt;
和上次一样，又是stack info全是？？。但是这次有所不同的，上次的dfs中stack问题是因为stack乱掉，虽然编译的时候加了-g参数，可执行文件保存了调试信息，但还是乱掉了，全是？。而这次的php是因为编译的时候没有加-g参数，所以调试信息压根就没有保存下来，所以在这里看不见到stack info是很正常的；&lt;/p&gt;

&lt;p&gt;还是借助寄存器吧，上次就是通过寄存器最后解决了问题，获取一下寄存器的值，如下图：  &lt;br /&gt;
&lt;img src=&quot;3.png&quot; alt=&quot;php regedits&quot; /&gt;&lt;/p&gt;

&lt;p&gt;再通过x命令看一下rbp之前的stack，如下图：  &lt;br /&gt;
&lt;img src=&quot;4.png&quot; alt=&quot;php stack&quot; /&gt;  &lt;br /&gt;
通过这个命令可以看到有3个标识出现了，分别是：zend_check_magic_method_implementation，ip_maskr和zif_sha1_file。因为对php很不熟悉，所以只能g一下php的源码，知道一下这3个到底是啥玩意。如下：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;zend_check_magic_method_implementation：这个是php的一个函数，主要用来做调用php函数之前的校验之类的用的，这个应该关系不大；&lt;/li&gt;
  &lt;li&gt;ip_maskr：这个是一个staic struct,在php的内核crypt_freesec.c文件中182行，大小为8*256，有2048b=2k啊，好大，有重点嫌疑；&lt;/li&gt;
  &lt;li&gt;zif_sha1_file： 这个是用来计算每个申请访问文件的hash值的，在这个函数中需要用到ip_maskr的值，这个也是有重大嫌疑；&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;分析&quot;&gt;分析&lt;/h2&gt;
&lt;p&gt;综上所述，好像发现了一点什么。看最后一张图，上面显示的是ip_maskr+4144，也就是说在ip_maskr的偏移4144处。ip_maskr一共就2k啊，指针的指向不对了，越界了，所以导致了程序crash。那么为什么会指针有问题呢？再要查一下。&lt;/p&gt;

&lt;p&gt;这里就和我们更新php的方式有关了。我们使用reload模式更新，难道是因为reload模式的问题？接触这个疑惑的办法就是我们去看一下php在执行reload的时候执行了哪些代码。所有的bug都是由代码导致的，所以看代码还是根本。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;首先，查看了一下php-pfm的代码，reload使用了SIGUSR2的信号，这个没有问题，大家都是这么玩的；&lt;/li&gt;
  &lt;li&gt;然后，查看SIGUSR2的信号回调函数，如下：&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    static void sig_soft_quit(int signo)
    {
        int saved_errno = errno;
        /* closing fastcgi listening socket will force fcgi_accept() exit immediately */
        close(0);
        if (0 &amp;gt; socket(AF_UNIX, SOCK_STREAM, 0)) {
            zlog(ZLOG_WARNING, &quot;failed to create a new socket&quot;);
       }
       fpm_php_soft_quit();
       errno = saved_errno;
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;问题基本上就是这里了，php确实关掉了监听的socket，但是已经连接了的socket呢，怎么处理了？应该就是因为没有关闭的连接继续执行而静态区的数据已经被破坏，内存映射出现了偏差导致的。g一下，发现php社区其实已经发现了这个问题，bug号：60961。有兴趣大家可以关注一下。可悲的是，我们不小心踩了雷。&lt;/p&gt;

&lt;p&gt;问题最后是找到了，但是上面cpu和磁盘都100%的问题又是什么原因呢？   &lt;br /&gt;
这个其实是牵连问题，也和php有关系。因为php使用fastcgi来处理web请求，执行之间使用父子进程模式，父进程监控子进程的健康状况和重启子进程；而子进程是多子进程，是真正执行处理的地方，我们的服务器上开了有300个子进程，php的客户端访问会被分配到每个执行的子进程上。然而悲剧的也就是子进程多的时候，当子进程crash的时候，我们又给系统开了core文件生成，所以这300个进程又同时写core文件，所以CPU和磁盘肯定都是100%的负载。&lt;/p&gt;

&lt;h2 id=&quot;解决办法&quot;&gt;解决办法&lt;/h2&gt;
&lt;p&gt;处理办法也很简单，堵住问题的引起点就行了。不要使用reload模式，而是先把服务器下线，然后确保没有连接后，使用restart方式重启php，再将更新完成的php上线就没有问题了。&lt;/p&gt;

</description>
        <pubDate>Tue, 11 Jul 2017 00:00:00 +0800</pubDate>
        <link>http://www.94geek.com/blog/2017/debug-php/</link>
        <guid isPermaLink="true">http://www.94geek.com/blog/2017/debug-php/</guid>
      </item>
    
      <item>
        <title>为什么这样设计Chaos</title>
        <description>&lt;p&gt;随着上一篇介绍Chaos的文章推送，最近有好几家公司或者项目负责人联系我，准备在生产环境中使用Chaos，所以也会经常被问到一些问题。这里我总结了一下经常会被问到的几个问题，给大家做一个统一的回答吧！&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;怎么样搭建Chaos的集群环境？&lt;br /&gt;
这个问题分为2步：
    &lt;ul&gt;
      &lt;li&gt;首先是配置mid：chaos的配置文件中有一个配置项是mid，这个mid就是每台chaos服务器的唯一标识，目前支持配置值是0-9的数字。只要在一个集群中保证每台chaos的mid不一样即可。&lt;/li&gt;
      &lt;li&gt;在chaos的前端放一个ngx或者是HA这种的带有“负载均衡”功能的反向代理服务器，将各自单台的chaos组合起来即可，我们用的是ngx；&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;chaos进程起来都是“回话模式”，退出term就没了，怎么设置“后台运行”，难道要自己加&amp;amp;？&lt;br /&gt;
chaos当然不会傻到让使用方自己加&amp;amp;来daemon化，chaos的配置文件中有一个配置项daemon，把这一项设置成true即可。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;chaos起不来，出现libev.so can not found的错误，这是怎么回事？&lt;br /&gt;
这是因为你虽然安装了libev，但是有的系统（比如centos）不会自动去更新系统的so缓存，所以你需要更新一下系统的so缓存。
    &lt;ul&gt;
      &lt;li&gt;执行命令：echo “/usr/local/lib” » /etc/ld.so.conf.d/x86_x64_linux_libev.conf ；&lt;/li&gt;
      &lt;li&gt;ldconfig -v
搞定。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;为什么chaos起来后本地可以访问，换台机器就不能访问了？&lt;br /&gt;
首先你要排除一下你的防火墙，iptables之类的配置是不是都已经开启chaos所使用的端口了。如果已经支持了，那么请看一下chaos配置文件中的配置项bind_ip这个项的值是多少，默认是127.0.0.1，请将这个配置项改成chaos服务器所在的外网可以访问的ip，重启chaos，你就可以在另外一台机器上获取id了。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;chaos在集群的情况下（特别是不同机器之间）生成的id是保证单调自增的吗？&lt;br /&gt;
不是，chaos生成的id在集群的情况下不保证严格的单调递增，特别是1s内生成的id，但chaos肯定保证2s内生成的id具有严格单调递增性，也就是说后一秒生成的id值肯定大于前一秒生成的id值。造成这个原因主要是因为几点：
    &lt;ul&gt;
      &lt;li&gt;严格单调递增不是不能做到，而是消耗的资源太多了。现在chaos在集群情况下是没有主从之分的，也没有相互之间的任何通讯，从而保证了我们获取一个id的快速性，也保证了chaos整个的简单性；&lt;/li&gt;
      &lt;li&gt;获取id后，我们经常做的一个动作就是分库分表操作，所以id会被根据业务规则散落到各个“段表”中，在这过程中，时间应该会在ms或者是s级别，所以chaos严格递增性其实没有那么大的实际需求，我们就可以不优先实现这个功能；&lt;/li&gt;
      &lt;li&gt;系统的健壮性。平行的chaos功能设计有利于chaos的部署简单化和提高chaos的系统鲁棒性，不管chaos的机器发生什么问题，就算机器down掉或者直接换掉，chaos都可以快速的通过使用新机器、新节点来提供新的功能，不需要恢复数据、操作日志（因为根本就没有，每次的id都是纯计算生成的）等等； &lt;br /&gt;
PS：我们有支持严格单调递增的id生成器，是另外一个系统：lax605。lax605在我们的系统中更多的被用来作为分布式协调器的角色使用，类似于zookeeper。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;chaos生成的id是否存在浪费？怎么处理？  &lt;br /&gt;
有浪费现象，比如现在每台机器每秒都可以支持最大2560000个id的生成，但是因为99.9999%的时候你根本就用不到那么多的id，真实的业务量大概每秒也就几百或者几千的id生成，从而因为id中存在序列号，所以没有被轮询到的id就直接被扔掉了，这确实有浪费现象。怎么处理？chaos就没有去处理，因为我们觉得这个没啥必要，反正每秒id那么多，而且对于业务来说如果分库分表规则是按照时间等作为路由是要严格保证时间上的正确性的，综上我们就不去过多的去做额外的处理了。但如果有业务方确实觉得id是一个虚缺资源，需要每一个都不放过，那么可以通过中继器来过渡一下，自己做一个每个id的存储和分发。但是在这么做之前，我觉得要冷静下来思考一个问题：这种做法真的有必要吗？&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;chaos生成的id最大值是多少？多长？长度固定吗？       &lt;br /&gt;
chaos生成的是一个uint64的数，最大的长度是64位，一共最长是18个数字的长度。但是因为id和时间相关，所以开始的时候id的个数长度会比较短，大概在15位左右。所以id的长度是不固定的。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;chaos的客户端有没有java的？&lt;br /&gt;
首先，chaos目前支持两种方式访问：tcp和http。java的客户端也确实有，并且也有开源（在albianj项目中），java的客户端就是通过tcp方式访问的。但就我们自己使用的实际情况和实际效果来看，我们不推荐使用java客户端的tcp方式，我们推荐大家更多的使用http来访问chaos，主要有几点：
    &lt;ul&gt;
      &lt;li&gt;http相比tcp更容易调试，curl或者是浏览器就可以直接调试；&lt;/li&gt;
      &lt;li&gt;http相比tcp，http没有语言的相关性需求，当一个公司或者项目大了以后不保证所有的功能都是java来实现的，所以客户端模式会很蛋疼的需要很多的客户端；&lt;/li&gt;
      &lt;li&gt;对于tcp的通讯相比http性能高的问题，chaos这种访问都是在内网，用http和tcp能差多少？&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;id生成器目前在公司内部使用的怎么样？  &lt;br /&gt;
目前公司内部的内容中心、起点改造等站点正在使用id生成器，每天的请求量总体加起来过亿，一共花费了4台普通的pc服务器。id生成器从上线的那天开始一直到今天有2年多了，中间除了因为功能增加以外从来没有down机，也没有重启记录。稳定性达到惊人的100%。可以算是公司内最稳定的线上运行项目，没有之一。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;目前有几家公司正在使用？  &lt;br /&gt;
到今天为止，一共有10+家公司已经在线上使用id生成器，目前来咨询有使用意向的也有7-8家。我们是最大的一家：阅文集团，该集团是腾讯文学、盛大文学合并而成的。请求量和数据量在网络文学届也是数一数二的。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;评价一下别的id生成器，和chaos有什么差别？ &lt;br /&gt;
这nm怎么评价，谁都是看自己家的孩子更好看，更可爱啊！不要搞事啊！我严重怀疑问我这个问题的人的居心，但一看问这个问题的还挺多。nm，果然都是来搞事的。不过话说回来，虽然我确实看了很多友商的id生成器设计或者是成品，各有千秋，各有优势吧！我觉得我们的chaos主要有几个优势：
    &lt;ul&gt;
      &lt;li&gt;在架构上更加的简单，平行化的设计，没有单点或者主从的问题，也没有机器的浪费问题，都是主；&lt;/li&gt;
      &lt;li&gt;依赖少，除了依赖网络库libev，没有什么mysql数据库或者redis这种依赖（PS：其实我也觉得很奇怪，为嘛一个id生成器需要依赖数据库或者是redis这种玩意，这种东西到底在id生成器中有啥用？除了复杂化设计外，没看到什么必要）；&lt;/li&gt;
      &lt;li&gt;性能上更牛X一些吧，c语言，事件机制，相比java之类的，那是相当妥妥的；&lt;/li&gt;
      &lt;li&gt;可读性上更人性化一些，十进制的数字，基本上都能看的懂；&lt;/li&gt;
      &lt;li&gt;协议更简单一些，支持http，无语言相关性；&lt;/li&gt;
      &lt;li&gt;更适合作为数据库的主键来使用，特别是需要做分布式数据库的情况，有分库分表等等数据路由的情况特别适合；&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 05 Jul 2017 00:00:00 +0800</pubDate>
        <link>http://www.94geek.com/blog/2017/chaos-qa/</link>
        <guid isPermaLink="true">http://www.94geek.com/blog/2017/chaos-qa/</guid>
      </item>
    
  </channel>
</rss>
