<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>94geek.com</title>
    <description>属于大嘴的个人blog</description>
    <link>http://www.94geek.com/</link>
    <atom:link href="http://www.94geek.com/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>mysql连接池不能回避的wait timeout问题</title>
        <description>&lt;p&gt;感谢我们的木木同学给了我写这篇文章的灵感和机会。&lt;/p&gt;

&lt;h2 id=&quot;起因&quot;&gt;起因&lt;/h2&gt;
&lt;p&gt;我们的项目组一直在使用albianj作为开发框架在开发应用。使用至今倒也是没有出现很大的问题，但最近加过监控的接口基本上都会在使用一段时间后，突然之间执行数据库操作变得很慢。虽然会变慢，但持续的时间比较短，一般1分钟左右，然后会自动恢复正常。但是过了一段时间，这个现象又会出现，周而复始。从监控看，发生的时间点并无规律，有的时候一天发生3次，有的也会有4-5次。虽然从规律上并无法去查找，那就只能从别的地方想办法：增加一些详细的日志，从日志上看一下问题所在。&lt;br /&gt;
详细日志版本刚刚上去，立刻就发生问题了。如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;3.png&quot; alt=&quot;problem&quot; /&gt;&lt;/p&gt;

&lt;p&gt;看一下左下角的曲线图，突然飙高，然后在1500ms的高位不下来。经过查找日志（PS：sorry。写文章的时候日志已经被自动清除，没法截图了），发现程序卡在了getConnection这个方法上，并且卡住的时间从60s开始越来越长。&lt;/p&gt;

&lt;h2 id=&quot;分析&quot;&gt;分析&lt;/h2&gt;
&lt;p&gt;getConnection是一个synchronized方法，主要是从连接池中获取数据连接！卡住时间越来越长这个现象倒是很简单就可以解释：因为getConnection是synchronized的，所以所有的线程到getConnection的时候全部等待，等待的时间越长当然时间越长了。关键在于为啥卡住呢？&lt;br /&gt;
当时有两种可能的想法：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;连接池设置的太小，连接在使用的时候来不及被返还，导致了积压；&lt;/li&gt;
  &lt;li&gt;连接池的设置有问题，返回的连接有问题。如果是这个问题，那可能会比较难查；&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;第一种情况显然不会存在，查看albianj的数据库配置，所有的配置对于连接池的设置MinSize为5，MaxSize为20.对于我们的应用来说肯定是够用的。那么就是第二个问题了。&lt;/p&gt;

&lt;p&gt;首先检查连接复用问题，对于数据库的连接字符串，我已经增加了重连的设置：&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;autoReconnect=true&amp;amp;failOverReadOnly=false&amp;amp;zeroDateTimeBehavior=convertToNull&amp;amp;maxReconnect=3&amp;amp;autoReconnectForPools=true
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;显然autoReconnect没有起作用。查了一下mysql的客户端说明，后背发凉。基本上的大意翻译过来是这样的&lt;strong&gt;“即使在创建Mysql时url中加入了autoReconnect=true参数,一但这个连接两次访问数据库的时间超出了服务器端wait_timeout的时间限制,还是会CommunicationsException: The last packet successfully received from the server was xxx milliseconds ago. ”&lt;/strong&gt;。赶快去查一下日志，发现并没有报这个错误。那应该不是这个问题。&lt;/p&gt;

&lt;p&gt;既然不能重连接，那么最大的可能就是&lt;strong&gt;“返回的连接本来可能就是有问题的，但是程序确认为没有问题”&lt;/strong&gt;。发生这种问题的最大可能应该就是：&lt;strong&gt;数据库连接池的连接过期时间大于mysql的wait_timeut设置&lt;/strong&gt;。查了一下代码，发现我们默认的数据库连接池连接过期时间是300-30=270s。再去问一下DBA木木，让他查一下线上的数据库wait_timeout设置，被告知是180.瞬间懵逼：连接池的生命周期时间远远参与真实的连接生命周期时间。&lt;/p&gt;

&lt;p&gt;原因是找到了，但是这个问题其实前一段时间已经发现并且已经改过了。因为去年我记得很清楚：又一次我们的probactor（job调度系统）报了上文提到的CommunicationsException异常，后来找到了问题就是因为程序对于连接池中连接的alivetime长于数据库的wait timeout设置，随后我千叮咛万嘱咐这个设置必须要小心小心再小心，但这次还是中招了。因为连接池和网络的问题，我们有同事直接放弃连接池，改用每次连接数据库。放弃连接池确实能解决连接不会出现问题，但是侧面也导致了wait_timeout被设置的过小了。但其实只要把连接池中连接的过期时间设置的比wait_timeout小一些就完全可以了。&lt;/p&gt;

&lt;p&gt;经过更改后的程序终于恢复了正常，看一下更改后的效果：&lt;br /&gt;
&lt;img src=&quot;2.png&quot; alt=&quot;ok&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;科普&quot;&gt;科普&lt;/h2&gt;
&lt;p&gt;何为wait_timeout？&lt;br /&gt;
wait_timeout是mysql的一个设置，主要是用来断开不使用的数据库连接。当连接空闲的时间达到wait_timeout设置的最大值时，mysql会主动切断这个连接，以供别的客户端连接数据库。这个值一般是28800，也就是8小时。在mysql中可以通过：
&lt;code class=&quot;highlighter-rouge&quot;&gt;show variables like “%timeout%”;&lt;/code&gt;
获取。&lt;br /&gt;
另外，当数据库主动切断连接的时候，java的mysql客户端并不知道这个连接已经被切断，所以程序并不知道其已经无效了，然后加上mysql的客户端不支持ReConnect，双重的问题叠加在一起就导致了连接池返回无效连接的可能。这是一个比较扯淡也是一个比较难以发现的问题，但是它确确实实的存在了。大家一定要多加注意。&lt;/p&gt;

&lt;p&gt;这个值可以根据自己网络的环境和业务的并发性来调整。&lt;/p&gt;
</description>
        <pubDate>Tue, 22 Aug 2017 00:00:00 +0800</pubDate>
        <link>http://www.94geek.com/blog/2017/mysql-wait-timeout/</link>
        <guid isPermaLink="true">http://www.94geek.com/blog/2017/mysql-wait-timeout/</guid>
      </item>
    
      <item>
        <title>台湾技术交流见闻与感想</title>
        <description>&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;

&lt;p&gt;这几天有幸因为台湾主办方和国内著名出品人史海峰的邀请前去台湾进行了技术交流。这次技术交流一方面让台湾那边了解目前国内互联网行业的一些技术和状态，另外一方面也让我们可以更加了解台湾互联网技术方面的情况。&lt;/p&gt;

&lt;p&gt;毕竟是来交流技术的，所以先说一下我们这次两岸技术交流的情况和我对于这次交流的一些心得和体会。&lt;/p&gt;

&lt;h2 id=&quot;技术交流&quot;&gt;技术交流&lt;/h2&gt;

&lt;p&gt;从技术交流的现场来说，台湾的技术交流场面很火爆。基本上座无虚席，还有没有抢到主会场的只能看直播。但架构场相对就没有那么理想了，原因后面会讲到。火爆的会场如图：
&lt;img src=&quot;7.jpeg&quot; alt=&quot;交流现场&quot; /&gt;
台湾目前的互联网技术水平和国内相比，说句实话：相差的有点远。在台湾，大多数的工程师都是FullStack类型，并不像是国内的互联网行业：不想当架构师的程序员不是好程序员。而且台湾的技术工作分工也没有国内细分的详细。在台湾最受欢迎的是前端，并不是架构师等后端职位。我们在架构场开讲的时候做过几个调查：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;在座的有多少个是写后端的？举手的只有3个左右，当然我们说的是纯后端。&lt;/li&gt;
  &lt;li&gt;这里有多少个title或者日常工作是架构师？ 举手的只有1个。追问：用什么语言？答：python。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;而我在交流的时候也问了几个问题，有技术也有业务：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;看网络小说的有多少？大概有20个人左右，人还是挺多的；&lt;/li&gt;
  &lt;li&gt;知道阅文集团/起点/腾讯文学任何一家的？前面举手的大概都知道。这里有个情况，其实我们的一些作品有在台湾播放电视剧，我注意一下台湾的电视台，我至少看见有3部：花千骨，芈月传，甄嬛传在播出。后来打听了一下，甄嬛传几乎被当成了“当时内地的还珠格格那种热度”在台湾播出。对我们来说，这个市场还是很大的；&lt;/li&gt;
  &lt;li&gt;技术上就又不一样了，比如讲分库分表和数据路由的时候，基本上下面是一片懵逼。在讲到后面调度系统的时候，quartz这个东西只有3个人听过，cron表达式也只有5个人听过；我调查了一下，写过定时任务的一个都没有。我就有点纳闷：他们的网站难道没有定时任务需要处理吗？&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;我们几个讲师其实也挺费劲，会前讨论应该怎么样让他们听的懂我们在讲什么？会后我们还是在讨论为什么出现这个情况，后来我们总结了一下，主要归结如下：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;用户。台湾一共2200w人，内地15亿。没有用户量爆发式增长，所以很多问题都碰不到，他们也就不用解决，像我们陈老师演讲的时候说，我们一开始规划下来需要26个数据库，马上下面的留言板上说，26个数据库，太庞大了，囧；&lt;/li&gt;
  &lt;li&gt;必要性。按照目前的机器硬件配置，在没有爆发式增长的情况下，没有架构也好，不要后端也罢，只要会写代码，拉几个开源的就可以上了。可能缓存都用不了，不要说什么读写分离、分库分表这些东西了；&lt;/li&gt;
  &lt;li&gt;国外的冲击。米国的互联网台湾都可以用，想fb，google这些都可以，畅通无阻，那么社交他们也主要用line，所以台湾根本没有自主的互联网企业和软件。在“5分钟演讲”环节，有个台湾本地哥们提出来一个问题：台湾有名的软件公司有哪个？只回答了一个“趋势科技”就没了，互联网技术公司一个都没有；&lt;/li&gt;
  &lt;li&gt;重视前端。因为碰不到用户量和数据量的关系，又因为大部分东西都是用国外的，所以剩下的能做的就没几样了。你不做前端做什么呢？&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;台湾的互联网技术除了这些之外，还有几个特点：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;脚本语言占大多数。我们在场内看到的最多的开发者是用nodejs，再下面就是go之类的新兴语言，也有python之类的脚本。但是国内很多互联网公司喜欢用的java，c/cxx这些语言在这里已经绝迹；&lt;/li&gt;
  &lt;li&gt;拼凑。几乎所有的开发都是用了开源的组件拼拼凑凑起来的。印象比较深的是一个哥们讲微服务，然而他并没有去讲微服务怎么怎么实现，而是讲了用了什么什么框架，什么什么组件，在什么什么上，搭起来了。就是那种纯粹的堆积木。我不知道如果这要是放在内地，会不会出去了要挨揍；&lt;/li&gt;
  &lt;li&gt;追新。什么新用什么，不会管这个新的东西到底怎么样，是不是适合，这些好像他们不太会去考虑。上面说到的开源使用，有一些就是觉得为了使用而使用，为了体现我们这个东西是多么的新潮，使用了多么cool的新技术就去使用了，而不会去规划和思考底层的问题；&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;这次的技术交流，对于我们内地来的讲师来说，最感同身受的是：&lt;strong&gt;我们的互联网技术确实已经起来了。说句大话，对手可能只有一个了，剩下的都是渣渣！&lt;/strong&gt;&lt;br /&gt;
对于台湾技术来说，要想真的提高，必须也要从商业模式和底层技术开始寻找出路，有了良好的商业模式，有用户开始喜欢使用你们的产品了，自然而然的对一些后端的技术会有更高的要求。技术自然就会跟着起来了。但就从现阶段来看，目前的台湾就像我们在10年前、20年前看硅谷一样：一切都是那么的新奇、牛逼闪闪而高高在上。&lt;/p&gt;

&lt;h2 id=&quot;台大&quot;&gt;台大&lt;/h2&gt;

&lt;p&gt;说完演讲正事，再说一下这次的举办场地。主办发选择了台湾大学的社科院作为举办场地让我受宠若惊。第一次站在学术氛围浓厚的大学进行演讲，难免还是有点紧张。演讲之余我们几个大陆来台的讲师也在台大转了一下，我有几个体会：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;台大的校门很低调，低调到如果是在路上走，你可能会根本没在意它的存在。
&lt;img src=&quot;2.jpeg&quot; alt=&quot;台大校门&quot; /&gt;
只有走进了看，才能看清楚上面的字。
&lt;img src=&quot;1.jpeg&quot; alt=&quot;台大校门详细&quot; /&gt;
怎么说台大也是出了很多名人的地方，不管是政治届、娱乐圈、工商界、科技界还是学术界，名人都是大把大把的不缺。但是这个校门真的太低调；&lt;/li&gt;
  &lt;li&gt;台大内部给我的感觉就像是一个缩小版的东京大学，不管是建筑风格、院系设置还是人文情怀，几乎都是照搬东大而来（PS：后来认识了一个导游哥们，他说台大以前叫帝国大学，是在日本占据台湾的时候兴建的，后来国军入台后才改名台大。回去一查，果然前几任校长都是日本人）。
&lt;img src=&quot;3.jpeg&quot; alt=&quot;图书馆&quot; /&gt;
&lt;img src=&quot;4.jpeg&quot; alt=&quot;台大校门详细&quot; /&gt;
这是国内的电子信息工程系，这里的名字叫什么来着，忘记了，反正挺长而且挺繁琐。
&lt;img src=&quot;5.jpeg&quot; alt=&quot;电子信息工程系&quot; /&gt;
没记错的话应该是日本留学生或者是日本留学生留下的一个雕塑作品（PS：这里可能记不太清了。这个雕塑还有一个是草地上的建筑模型，这两个不知道哪个的作者是日本人了。有点健忘，不好意思）
&lt;img src=&quot;6.jpeg&quot; alt=&quot;地标：阅读的小女孩&quot; /&gt;
这个是台大的图腾。一个钟。上面写着：一天其实只有21个小时，另外3个小时是用来……
&lt;img src=&quot;8.jpeg&quot; alt=&quot;台大的图腾&quot; /&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;彩蛋&quot;&gt;彩蛋&lt;/h2&gt;
&lt;p&gt;程序员就是程序员，会后主办方安排我们去领略了一下台湾当地的风俗人情。其中有个放天灯的环节，一般人在天灯上无非都留下些什么“身体健康”，“全家幸福”，“xxx爱你一辈子”这种的话。只有我们程序员的天灯是那么的与众不同：&lt;/p&gt;

&lt;p&gt;hello world。其实这个不是我写的，虽然拍的是我。我写的是 root#:rm -rf /
&lt;img src=&quot;9.jpeg&quot; alt=&quot;hello world&quot; /&gt;&lt;/p&gt;

&lt;p&gt;来自前端兄弟的javascript是世界上最好的语言。其实我是想用c反驳的，但是家伙狡猾狡猾的，全写满了，不给机会！
&lt;img src=&quot;10.jpeg&quot; alt=&quot;javascript是世界上最好的语言&quot; /&gt;&lt;/p&gt;

&lt;p&gt;58沈大师的“当场面试”–快排算法。可惜最后“一致不怀好意的”判定有两个问题：第一个，函数没写全，编译不通过；第二个，递归漏写了退出条件，CPU100%。
&lt;img src=&quot;11.jpeg&quot; alt=&quot;快排&quot; /&gt;&lt;/p&gt;

&lt;p&gt;主办方和我们架构场的讲师们
&lt;img src=&quot;12.jpeg&quot; alt=&quot;我们架构场的讲师&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这是我们大陆的所有讲师在101大楼，101大楼发生了很多故事，但是只能发这一张了。大陆讲师就缺钟恒了，家伙跑出去单独行动了，目的比较可疑。
&lt;img src=&quot;13.jpeg&quot; alt=&quot;我们讲师&quot; /&gt;&lt;/p&gt;

</description>
        <pubDate>Sun, 13 Aug 2017 00:00:00 +0800</pubDate>
        <link>http://www.94geek.com/blog/2017/taiwan-ithome/</link>
        <guid isPermaLink="true">http://www.94geek.com/blog/2017/taiwan-ithome/</guid>
      </item>
    
      <item>
        <title>网络read函数未判返回0导致CPU 100%</title>
        <description>&lt;p&gt;我们的“运维小帅哥”又来烦我们了！没事就在群里给我们post了一张图，如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;1.png&quot; alt=&quot;show&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;新上线的lest系统的CPU使用率很高！&lt;/strong&gt;。一看才17%，不是正常吗？小帅哥爆发了，不对。这是多核，被平均的消耗！实际上两个核已经被吃满了！&lt;/p&gt;
&lt;h2 id=&quot;检查服务器&quot;&gt;检查服务器&lt;/h2&gt;

&lt;p&gt;使用top命令看了一下，如下图：&lt;br /&gt;
&lt;img src=&quot;2.png&quot; alt=&quot;top&quot; /&gt;&lt;/p&gt;

&lt;p&gt;从上图可以看到，cpu的load负载一直在2左右，也就是说其中的2个核已经被占满。再看下面，是被2个cpu沾满的！看一下CPU的情况，其中系统负载和用户负载几乎是一样的，一个是9.1%，另外一个是7.4%。这就是说可能的问题出在用户态，用户态因为需要调用系统调用，把系统的负载带起来了！那么造成这种疯狂消耗CPU的原因基本上99%都是在循环中干了什么傻事，导致循环出不来，疯狂的消耗CPU！&lt;/p&gt;

&lt;p&gt;因为是CPU负载比较高，不是什么内存泄露之类的问题，所以没办法通过core来精确的debug。我们只能靠gcore通过人工的方式强行抓取进程的runtimes碰碰运气，看看是不是能看出来一点蛛丝马迹。&lt;/p&gt;

&lt;h2 id=&quot;debug看runtimes&quot;&gt;debug，看runtimes&lt;/h2&gt;

&lt;p&gt;首先，加入core后得到的结果如下图：&lt;br /&gt;
&lt;img src=&quot;3.png&quot; alt=&quot;gdb core&quot; /&gt;&lt;/p&gt;

&lt;p&gt;和上面top对应的是两个线程：25128和24130。因为问题就是出在它们身上，我们进入这2个线程看一下它们到底在做什么，首先看一下线程的编号，如下图：&lt;br /&gt;
&lt;img src=&quot;4.png&quot; alt=&quot;th1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;当前线程就是28这个，那就直接来吧，运行bt，如下图：&lt;br /&gt;
&lt;img src=&quot;7.png&quot; alt=&quot;bt&quot; /&gt;
和前面的几次一样都是啥都没有的？，还是使用%rbp来看一下吧，如下图：&lt;br /&gt;
&lt;img src=&quot;5.png&quot; alt=&quot;rbp&quot; /&gt;
这是在读取数据，这个也是正常的调用，但是它是一个“可能的循环”。看上去没发现什么有价值的东西。那么进入到30这个线程看看，如下图：&lt;br /&gt;
&lt;img src=&quot;6.png&quot; alt=&quot;th30&quot; /&gt;
这个线程还是在stack的顶端，没在运行。也就是说可能在我们抓取core的一瞬间，此线程正好因为cpu的时间片被让出，啥都没在干。&lt;/p&gt;

&lt;p&gt;到目前为止，通过gdb我们没有发现很多有价值的东西，除了那个疑似的可能性循环，但是就这点信息不能确定就是它的问题。也就是说从用户态入手，我们看不太到我们想要的信息，那我们看看系统调用的。&lt;/p&gt;

&lt;h2 id=&quot;再次求证&quot;&gt;再次求证&lt;/h2&gt;

&lt;p&gt;使用strace将所有线程的系统调用全部抓取出来，因为线程太多了，容易形成干扰，所以我们使用grep将怀疑有问题的25128线程给抓取出来，如下图：&lt;br /&gt;
&lt;img src=&quot;8.png&quot; alt=&quot;strace&quot; /&gt;&lt;/p&gt;

&lt;p&gt;为了明确问题，我已经用红色的标记给标出来了！我们发现在整个的系统调用中，一直在循环的调用read，epoll_ctl，这样的函数。再仔细看一下，每次read都欲获取29长度的值（29是header的buffer长度），但是实际得到的长度是0，表示没有数据。也就是说，我们试图每次都获取数据，但每次都没有获取到数据，事件机制又把这个fd给重新加到了epoll中监听了。回忆上面core在gdb中的表现，28线程一直在read数据。那么问题就在这里了。因为没有给read进行合理的返回值过滤，导致错误的返回值也被按照正确对待，重新加入epoll监听，有因为epoll是一个无线循环，而每次又都触发read，所以轻轻松松的就干掉了CPU。&lt;/p&gt;

&lt;h2 id=&quot;检查代码&quot;&gt;检查代码&lt;/h2&gt;

&lt;p&gt;回过头来检查代码，read的网络事件都是被组织在spx（是我们的一个c开发组件）中的，查看nio，如下图：
&lt;img src=&quot;9.png&quot; alt=&quot;code&quot; /&gt;
确实没有对read的真实获取长度（就是代码中的&amp;amp;len）为0的情况进行判断，所以发生了这个问题。解决办法也简单，将这个判断加上就可以了。，如下图：&lt;br /&gt;
&lt;img src=&quot;10.png&quot; alt=&quot;code&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;连带问题及其解决&quot;&gt;连带问题及其解决&lt;/h2&gt;

&lt;p&gt;解决问题，心里美滋滋的。上uat，观察一会儿！突然，我同事和我说，为什么fixed后的版本上传数据失败率有点高啊？多高？50%以上！我x。马上拿一个日志下来看看，如图：
&lt;img src=&quot;11.png&quot; alt=&quot;log&quot; /&gt;&lt;/p&gt;

&lt;p&gt;哎呀，很多重试的链接都被强行关闭了。仔细看一下上面的代码，也就是说当spx_read_to_msg_once的err为EAGIN的时候，len也是0，但是代码却先判断了0==len，所以就“变向”过滤掉了err == EAGIN的情况。但是err == EAGIN的情况是正常的，应该被再次加入epoll重试，所以调整一下代码，将0 == len的判断移到判断err的后面，先判断err，就不会出现这种情况下了，代码更改如下：&lt;br /&gt;
&lt;img src=&quot;12.png&quot; alt=&quot;code&quot; /&gt;&lt;/p&gt;

&lt;p&gt;这样世界终于清静了！上uat，果然畅通，稳定后再上到online，看一下这个cpu的曲线：
&lt;img src=&quot;13.png&quot; alt=&quot;cpu&quot; /&gt;&lt;/p&gt;

&lt;p&gt;在这个40°c的天气终于透心凉了！&lt;/p&gt;

&lt;h2 id=&quot;问题引申&quot;&gt;问题引申&lt;/h2&gt;

&lt;p&gt;那么还有问题来了，为什么write数据的时候，如果返回值为0不需要判断呢？&lt;br /&gt;
其实这个和tcpip的api有关。当read的时候，如果返回值是0，就表示对端已经关闭，所以后续没有数据读取了。write的时候，write的是本地的网络缓冲区，也就是滑动窗口，如果你的额client是一个慢client，读取你滑动窗口的数据特别慢，就会导致你write的时候数据写不进去，但是数据写不进去并不是一个错误，只是要你等client读取过后再试一次而已。所以write返回0加入epoll来使用事件触发是正确的。&lt;/p&gt;
</description>
        <pubDate>Tue, 08 Aug 2017 00:00:00 +0800</pubDate>
        <link>http://www.94geek.com/blog/2017/read-rc-0/</link>
        <guid isPermaLink="true">http://www.94geek.com/blog/2017/read-rc-0/</guid>
      </item>
    
      <item>
        <title>解决锁抢占问题--随机式获取抢占锁</title>
        <description>&lt;h2 id=&quot;背景&quot;&gt;背景&lt;/h2&gt;
&lt;p&gt;我们原本的调度系统是由quartz为基准DIY的系统，但因为quartz的很多问题，特别是可扩展设计是在太差、自定义功能太麻烦，我们不得不自行设计了一个调度系统，内部称为：probactr。probactr分为下面几个节点：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;monitor：监视器，主要负责监视Executor的状态和Executor执行的job状态，如发现Executor出现down机或者job出现问题，会对其进行清理。此节点为可平行扩展集群；&lt;/li&gt;
  &lt;li&gt;Executor：运行器，主要负责从数据库中获取欲执行的job，然后执行job。此节点为可平行扩展集群；&lt;/li&gt;
  &lt;li&gt;LockerServer：分布式锁服务器，为probactr提供一致性功能。目前使用redis替代，有计划将其替换成我们自主研发的lax605；&lt;/li&gt;
  &lt;li&gt;ManagerSite：后台管理系统，可以在这里对job进行添加、删除、暂停等等的管理，也可以查看job的执行状态；&lt;/li&gt;
  &lt;li&gt;database：数据库，所有的job数据全部存入数据库；&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;系统结构图如下：
&lt;img src=&quot;1.png&quot; alt=&quot;probactr&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;问题表现&quot;&gt;问题表现&lt;/h2&gt;
&lt;p&gt;probactr在开发环境中没有任何的问题，运行一切正常。上到test环境运行一段时间后发现有几个问题：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;可并行job（job分为可并行和不可并行2种）的统计状态不对；&lt;/li&gt;
  &lt;li&gt;打开邮箱发现报警邮箱已经被塞爆，据报警信息可知：更新job的状态和统计信息失败，并且基本上1s内可以产生3-4封同样的mail；&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;分析问题&quot;&gt;分析问题&lt;/h2&gt;
&lt;ol&gt;
  &lt;li&gt;首先，分析一下报警的mail，除了知道是更新状态和统计信息失败以外，还发现一个问题：所有报警的job都是可并行的job，且同一个trigger触发了很多个job；&lt;/li&gt;
  &lt;li&gt;接着，看一下job的Executor机器监控，发现CPU很高，有一些核能飙到100%；&lt;/li&gt;
  &lt;li&gt;然后，再查看一下数据库，发现被报警的trigger同时运行的job数特别大（我们当时没有对同一个trigger可以触发的job数进行限制）；&lt;/li&gt;
  &lt;li&gt;再查看一下别的job，发现一些不可并行的trigger并没有被触发，都被积压了；&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;到这里问题已经很清楚了，出问题的应该是可并行job导致的。首先想到的是可能是&lt;strong&gt;我们没有限制可并行的job数&lt;/strong&gt;，导致了可并行job并发特别厉害，我们设置阀值，应该就没有问题了。&lt;/p&gt;

&lt;p&gt;我们增加了这个功能，并且上test。发现确实问题减轻了很多，但是并不能完全杜绝。还是会有同样的报警mail出现，只是数量上少了很多。这可以证明问题只是得到了缓解，而并没有彻底解决。所以我们再去找原因。更新job状态的代码只有2处：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;job在启动的时候：trigger被scher扫描到，并且scher认为trigger满足被触发的条件。probactr会执行以下路径的代码：获取job的locker-&amp;gt;启动job-&amp;gt;更新job的状态-&amp;gt;释放locker；&lt;/li&gt;
  &lt;li&gt;job执行完毕的时候：job执行完毕后，也会执行上面同样的逻辑；&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;但是问题出现的位置应该不会是在job启动时，因为我们对于job的启动进行了weak化，job起不来没关系，下次scher扫描到再起来即可。但是在job结束时却是要强制性的，因为这一步job已经执行完成，必须要更新job的状态和信息，所以必须要成功，否则整个probactr的状态就会出问题。&lt;/p&gt;

&lt;p&gt;找到问题就好办了。&lt;/p&gt;

&lt;p&gt;再排查下去，更新数据库应该没有问题，因为就一条特别简单的sql语句，唯一能出现问题的地方应该是获取锁了。在job执行完成后，我们需要同时更新job和trigger的状态，所以必须要先获取locker。那么如果可并行job在同一时间结束，而且在同一时间去获取locker，确实可能会出现获取不到锁，然后相当于一直阻塞的状态。我们把并发job的数量改小，相当于同一时间获取的locker的请求量变小了，但是不保证一定没有，所以看起来问题减轻了。但这一步获取locker必须成功，所以保不齐肯定会磕到门牙，问题还是存在。那么为什么有时候CPU会飙升呢？因为我们必须要获取锁，所以一直在loop这个获取锁的功能，CPU当然撑不住。&lt;/p&gt;

&lt;h2 id=&quot;解决办法&quot;&gt;解决办法&lt;/h2&gt;
&lt;p&gt;首先想到的解决办法是：能不能放弃最后一步获取locker。但是很遗憾，经过一遍代码回溯发现行不通。不要locker的话还是存在数据不一致问题，job的属性状态和job的统计信息会不对；  &lt;br /&gt;
第二：调整获取锁的算法，使用sleep的方式来进行，比如sleep(100ms),这个方法在一定程度上是可以的，但还是会有问题，如果两个job同时抢占locker，如果sleep的时间一样，除了cpu的切换以外，还有一定的概率会第二次，第三次同时被唤醒；  &lt;br /&gt;
最后：我们使用了随机值的算法，在一定的范围内，根据我们的算法生成一个值，然后sleep这个随机值。这样可以巧妙的规避掉同时获取locker失败的问题。&lt;/p&gt;
</description>
        <pubDate>Tue, 18 Jul 2017 00:00:00 +0800</pubDate>
        <link>http://www.94geek.com/blog/2017/staircase-locker/</link>
        <guid isPermaLink="true">http://www.94geek.com/blog/2017/staircase-locker/</guid>
      </item>
    
      <item>
        <title>PHP&quot;伪司机&quot;调试PHP CORE</title>
        <description>&lt;p&gt;这次不是装逼，是真的帮忙找问题。对于php，一脸懵逼啊！因为就从来没写过，根本不懂php！&lt;/p&gt;

&lt;h2 id=&quot;发车&quot;&gt;发车&lt;/h2&gt;
&lt;p&gt;正常发布日，因为dfs的php客户端需要增加api而更新，就是在更新的过程中发生了问题，具体的表象为：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;程序无响应，访问web网页没有显示，直接报无响应；&lt;/li&gt;
  &lt;li&gt;CPU100%，服务器的CPU一直100%，非常“稳定”；&lt;/li&gt;
  &lt;li&gt;磁盘100%，磁盘监控显示也是100%，但是我们没有任何的磁盘操作啊，除了读php的文件，懵逼ing；&lt;/li&gt;
  &lt;li&gt;机器非常慢，执行命令非常慢，慢到无法忍受，甚至打命令都很一个字母一个字母的延迟；&lt;br /&gt;
更加诡异的是虽然我们确实更新了php插件，但是这个插件属于提前更新，业务代码并没有使用到这个插件新的api，所以首先排除是因为新插件新增功能的bug导致的，而老的api我们已经用了一年多了，也不会出现问题。那么只有更新的方式不对了？但是这个更新的方式也用了2年多了，以前一直没问题，怎么就今天出现问题了呢？幸好我们打开了core，php进程在crash的时候生成了core，我们可以用gdb调试一下。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;开车&quot;&gt;开车&lt;/h2&gt;
&lt;p&gt;还是老路子，首先压入core文件，如下图：
&lt;img src=&quot;1.png&quot; alt=&quot;php core&quot; /&gt;  &lt;br /&gt;
从这个图中我们可以得到2个信息，红色的已经标注：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;首先core是由php-fpm引起的，问过同事才知道这个是一个php提供web的组件；&lt;/li&gt;
  &lt;li&gt;生成core是因为信号11，也就是段错误。引起这个问题一般的问题就是内存错误，比如内存没有释放，使用了野指针，或者是溢出等等；&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;导入php的可执行文件，执行bt查看一下当前的stack情况，如下图：&lt;br /&gt;
&lt;img src=&quot;2.png&quot; alt=&quot;php bt&quot; /&gt;  &lt;br /&gt;
和上次一样，又是stack info全是？？。但是这次有所不同的，上次的dfs中stack问题是因为stack乱掉，虽然编译的时候加了-g参数，可执行文件保存了调试信息，但还是乱掉了，全是？。而这次的php是因为编译的时候没有加-g参数，所以调试信息压根就没有保存下来，所以在这里看不见到stack info是很正常的；&lt;/p&gt;

&lt;p&gt;还是借助寄存器吧，上次就是通过寄存器最后解决了问题，获取一下寄存器的值，如下图：  &lt;br /&gt;
&lt;img src=&quot;3.png&quot; alt=&quot;php regedits&quot; /&gt;&lt;/p&gt;

&lt;p&gt;再通过x命令看一下rbp之前的stack，如下图：  &lt;br /&gt;
&lt;img src=&quot;4.png&quot; alt=&quot;php stack&quot; /&gt;  &lt;br /&gt;
通过这个命令可以看到有3个标识出现了，分别是：zend_check_magic_method_implementation，ip_maskr和zif_sha1_file。因为对php很不熟悉，所以只能g一下php的源码，知道一下这3个到底是啥玩意。如下：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;zend_check_magic_method_implementation：这个是php的一个函数，主要用来做调用php函数之前的校验之类的用的，这个应该关系不大；&lt;/li&gt;
  &lt;li&gt;ip_maskr：这个是一个staic struct,在php的内核crypt_freesec.c文件中182行，大小为8*256，有2048b=2k啊，好大，有重点嫌疑；&lt;/li&gt;
  &lt;li&gt;zif_sha1_file： 这个是用来计算每个申请访问文件的hash值的，在这个函数中需要用到ip_maskr的值，这个也是有重大嫌疑；&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;分析&quot;&gt;分析&lt;/h2&gt;
&lt;p&gt;综上所述，好像发现了一点什么。看最后一张图，上面显示的是ip_maskr+4144，也就是说在ip_maskr的偏移4144处。ip_maskr一共就2k啊，指针的指向不对了，越界了，所以导致了程序crash。那么为什么会指针有问题呢？再要查一下。&lt;/p&gt;

&lt;p&gt;这里就和我们更新php的方式有关了。我们使用reload模式更新，难道是因为reload模式的问题？接触这个疑惑的办法就是我们去看一下php在执行reload的时候执行了哪些代码。所有的bug都是由代码导致的，所以看代码还是根本。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;首先，查看了一下php-pfm的代码，reload使用了SIGUSR2的信号，这个没有问题，大家都是这么玩的；&lt;/li&gt;
  &lt;li&gt;然后，查看SIGUSR2的信号回调函数，如下：&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    static void sig_soft_quit(int signo)
    {
        int saved_errno = errno;
        /* closing fastcgi listening socket will force fcgi_accept() exit immediately */
        close(0);
        if (0 &amp;gt; socket(AF_UNIX, SOCK_STREAM, 0)) {
            zlog(ZLOG_WARNING, &quot;failed to create a new socket&quot;);
       }
       fpm_php_soft_quit();
       errno = saved_errno;
    }
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;问题基本上就是这里了，php确实关掉了监听的socket，但是已经连接了的socket呢，怎么处理了？应该就是因为没有关闭的连接继续执行而静态区的数据已经被破坏，内存映射出现了偏差导致的。g一下，发现php社区其实已经发现了这个问题，bug号：60961。有兴趣大家可以关注一下。可悲的是，我们不小心踩了雷。&lt;/p&gt;

&lt;p&gt;问题最后是找到了，但是上面cpu和磁盘都100%的问题又是什么原因呢？   &lt;br /&gt;
这个其实是牵连问题，也和php有关系。因为php使用fastcgi来处理web请求，执行之间使用父子进程模式，父进程监控子进程的健康状况和重启子进程；而子进程是多子进程，是真正执行处理的地方，我们的服务器上开了有300个子进程，php的客户端访问会被分配到每个执行的子进程上。然而悲剧的也就是子进程多的时候，当子进程crash的时候，我们又给系统开了core文件生成，所以这300个进程又同时写core文件，所以CPU和磁盘肯定都是100%的负载。&lt;/p&gt;

&lt;h2 id=&quot;解决办法&quot;&gt;解决办法&lt;/h2&gt;
&lt;p&gt;处理办法也很简单，堵住问题的引起点就行了。不要使用reload模式，而是先把服务器下线，然后确保没有连接后，使用restart方式重启php，再将更新完成的php上线就没有问题了。&lt;/p&gt;

</description>
        <pubDate>Tue, 11 Jul 2017 00:00:00 +0800</pubDate>
        <link>http://www.94geek.com/blog/2017/debug-php/</link>
        <guid isPermaLink="true">http://www.94geek.com/blog/2017/debug-php/</guid>
      </item>
    
      <item>
        <title>为什么这样设计Chaos</title>
        <description>&lt;p&gt;随着上一篇介绍Chaos的文章推送，最近有好几家公司或者项目负责人联系我，准备在生产环境中使用Chaos，所以也会经常被问到一些问题。这里我总结了一下经常会被问到的几个问题，给大家做一个统一的回答吧！&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;怎么样搭建Chaos的集群环境？&lt;br /&gt;
这个问题分为2步：
    &lt;ul&gt;
      &lt;li&gt;首先是配置mid：chaos的配置文件中有一个配置项是mid，这个mid就是每台chaos服务器的唯一标识，目前支持配置值是0-9的数字。只要在一个集群中保证每台chaos的mid不一样即可。&lt;/li&gt;
      &lt;li&gt;在chaos的前端放一个ngx或者是HA这种的带有“负载均衡”功能的反向代理服务器，将各自单台的chaos组合起来即可，我们用的是ngx；&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;chaos进程起来都是“回话模式”，退出term就没了，怎么设置“后台运行”，难道要自己加&amp;amp;？&lt;br /&gt;
chaos当然不会傻到让使用方自己加&amp;amp;来daemon化，chaos的配置文件中有一个配置项daemon，把这一项设置成true即可。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;chaos起不来，出现libev.so can not found的错误，这是怎么回事？&lt;br /&gt;
这是因为你虽然安装了libev，但是有的系统（比如centos）不会自动去更新系统的so缓存，所以你需要更新一下系统的so缓存。
    &lt;ul&gt;
      &lt;li&gt;执行命令：echo “/usr/local/lib” » /etc/ld.so.conf.d/x86_x64_linux_libev.conf ；&lt;/li&gt;
      &lt;li&gt;ldconfig -v
搞定。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;为什么chaos起来后本地可以访问，换台机器就不能访问了？&lt;br /&gt;
首先你要排除一下你的防火墙，iptables之类的配置是不是都已经开启chaos所使用的端口了。如果已经支持了，那么请看一下chaos配置文件中的配置项bind_ip这个项的值是多少，默认是127.0.0.1，请将这个配置项改成chaos服务器所在的外网可以访问的ip，重启chaos，你就可以在另外一台机器上获取id了。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;chaos在集群的情况下（特别是不同机器之间）生成的id是保证单调自增的吗？&lt;br /&gt;
不是，chaos生成的id在集群的情况下不保证严格的单调递增，特别是1s内生成的id，但chaos肯定保证2s内生成的id具有严格单调递增性，也就是说后一秒生成的id值肯定大于前一秒生成的id值。造成这个原因主要是因为几点：
    &lt;ul&gt;
      &lt;li&gt;严格单调递增不是不能做到，而是消耗的资源太多了。现在chaos在集群情况下是没有主从之分的，也没有相互之间的任何通讯，从而保证了我们获取一个id的快速性，也保证了chaos整个的简单性；&lt;/li&gt;
      &lt;li&gt;获取id后，我们经常做的一个动作就是分库分表操作，所以id会被根据业务规则散落到各个“段表”中，在这过程中，时间应该会在ms或者是s级别，所以chaos严格递增性其实没有那么大的实际需求，我们就可以不优先实现这个功能；&lt;/li&gt;
      &lt;li&gt;系统的健壮性。平行的chaos功能设计有利于chaos的部署简单化和提高chaos的系统鲁棒性，不管chaos的机器发生什么问题，就算机器down掉或者直接换掉，chaos都可以快速的通过使用新机器、新节点来提供新的功能，不需要恢复数据、操作日志（因为根本就没有，每次的id都是纯计算生成的）等等； &lt;br /&gt;
PS：我们有支持严格单调递增的id生成器，是另外一个系统：lax605。lax605在我们的系统中更多的被用来作为分布式协调器的角色使用，类似于zookeeper。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;chaos生成的id是否存在浪费？怎么处理？  &lt;br /&gt;
有浪费现象，比如现在每台机器每秒都可以支持最大2560000个id的生成，但是因为99.9999%的时候你根本就用不到那么多的id，真实的业务量大概每秒也就几百或者几千的id生成，从而因为id中存在序列号，所以没有被轮询到的id就直接被扔掉了，这确实有浪费现象。怎么处理？chaos就没有去处理，因为我们觉得这个没啥必要，反正每秒id那么多，而且对于业务来说如果分库分表规则是按照时间等作为路由是要严格保证时间上的正确性的，综上我们就不去过多的去做额外的处理了。但如果有业务方确实觉得id是一个虚缺资源，需要每一个都不放过，那么可以通过中继器来过渡一下，自己做一个每个id的存储和分发。但是在这么做之前，我觉得要冷静下来思考一个问题：这种做法真的有必要吗？&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;chaos生成的id最大值是多少？多长？长度固定吗？       &lt;br /&gt;
chaos生成的是一个uint64的数，最大的长度是64位，一共最长是18个数字的长度。但是因为id和时间相关，所以开始的时候id的个数长度会比较短，大概在15位左右。所以id的长度是不固定的。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;chaos的客户端有没有java的？&lt;br /&gt;
首先，chaos目前支持两种方式访问：tcp和http。java的客户端也确实有，并且也有开源（在albianj项目中），java的客户端就是通过tcp方式访问的。但就我们自己使用的实际情况和实际效果来看，我们不推荐使用java客户端的tcp方式，我们推荐大家更多的使用http来访问chaos，主要有几点：
    &lt;ul&gt;
      &lt;li&gt;http相比tcp更容易调试，curl或者是浏览器就可以直接调试；&lt;/li&gt;
      &lt;li&gt;http相比tcp，http没有语言的相关性需求，当一个公司或者项目大了以后不保证所有的功能都是java来实现的，所以客户端模式会很蛋疼的需要很多的客户端；&lt;/li&gt;
      &lt;li&gt;对于tcp的通讯相比http性能高的问题，chaos这种访问都是在内网，用http和tcp能差多少？&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;id生成器目前在公司内部使用的怎么样？  &lt;br /&gt;
目前公司内部的内容中心、起点改造等站点正在使用id生成器，每天的请求量总体加起来过亿，一共花费了4台普通的pc服务器。id生成器从上线的那天开始一直到今天有2年多了，中间除了因为功能增加以外从来没有down机，也没有重启记录。稳定性达到惊人的100%。可以算是公司内最稳定的线上运行项目，没有之一。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;目前有几家公司正在使用？  &lt;br /&gt;
到今天为止，一共有10+家公司已经在线上使用id生成器，目前来咨询有使用意向的也有7-8家。我们是最大的一家：阅文集团，该集团是腾讯文学、盛大文学合并而成的。请求量和数据量在网络文学届也是数一数二的。&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;评价一下别的id生成器，和chaos有什么差别？ &lt;br /&gt;
这nm怎么评价，谁都是看自己家的孩子更好看，更可爱啊！不要搞事啊！我严重怀疑问我这个问题的人的居心，但一看问这个问题的还挺多。nm，果然都是来搞事的。不过话说回来，虽然我确实看了很多友商的id生成器设计或者是成品，各有千秋，各有优势吧！我觉得我们的chaos主要有几个优势：
    &lt;ul&gt;
      &lt;li&gt;在架构上更加的简单，平行化的设计，没有单点或者主从的问题，也没有机器的浪费问题，都是主；&lt;/li&gt;
      &lt;li&gt;依赖少，除了依赖网络库libev，没有什么mysql数据库或者redis这种依赖（PS：其实我也觉得很奇怪，为嘛一个id生成器需要依赖数据库或者是redis这种玩意，这种东西到底在id生成器中有啥用？除了复杂化设计外，没看到什么必要）；&lt;/li&gt;
      &lt;li&gt;性能上更牛X一些吧，c语言，事件机制，相比java之类的，那是相当妥妥的；&lt;/li&gt;
      &lt;li&gt;可读性上更人性化一些，十进制的数字，基本上都能看的懂；&lt;/li&gt;
      &lt;li&gt;协议更简单一些，支持http，无语言相关性；&lt;/li&gt;
      &lt;li&gt;更适合作为数据库的主键来使用，特别是需要做分布式数据库的情况，有分库分表等等数据路由的情况特别适合；&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Wed, 05 Jul 2017 00:00:00 +0800</pubDate>
        <link>http://www.94geek.com/blog/2017/chaos-qa/</link>
        <guid isPermaLink="true">http://www.94geek.com/blog/2017/chaos-qa/</guid>
      </item>
    
      <item>
        <title>gdb线上crash调试-记一次装逼失败的教训</title>
        <description>&lt;p&gt;可惜这是一次完完全全的装逼不成反被X的典型例子。&lt;/p&gt;

&lt;h2 id=&quot;背景&quot;&gt;背景&lt;/h2&gt;
&lt;p&gt;有一天早晨刚到公司，运维小帅哥突然通知我，我们的dfs在平稳运行了一年时间后，在没有任何异常波动、没有任何资源报警、没有任何升级，也没有任何违规操作的情况下，毫无征兆的crash了。&lt;br /&gt;
小帅哥一头雾水，我一脸懵逼。&lt;br /&gt;
dfs是我们的核心服务，绝对不能down机的，所以就先紧急把它拉了起来。命好的是，起来后程序又平稳运行了，既不crash也不拒绝服务，服务和性能又都是杠杠的。这就让我更懵逼了。&lt;/p&gt;

&lt;h2 id=&quot;怀疑&quot;&gt;怀疑&lt;/h2&gt;
&lt;p&gt;首先怀疑的是内存泄漏一类的资源泄漏问题。毕竟dfs是c写的，内存、fd、locker等等的资源处理太多太杂了。特别是内存，几乎每个函数内都会出现。所以重点对象就是它。&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;找运维查看一下报警系统是不是有没有发出来的报警？查了一下没有。&lt;/li&gt;
  &lt;li&gt;自己还是不放心，直接看了一下线上产生的core文件，大小也只有mb级别，确定确实没有溢出。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;接下来，怀疑是fd或者locker。&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;先说locker。dfs是我们自己设计的无锁处理算法，根本不存在locker的资源操作，所以这个也被排除了。&lt;/li&gt;
  &lt;li&gt;fd，文件描述符。如果fd有溢出，我们也对fd加了报警，报警系统是不会不报的。就算漏了，fd溢出的现象也不应该是进程直接crash，而是会无响应才对。而且去查看了一下log，并没有发现执行close有错误的迹象。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;到目前为止，所有所能想到的原因全部不成立，线索全部中断。那么如果非要弄清楚这次crash到底为了什么，唯一的办法就是去看程序的runtimes。&lt;/p&gt;

&lt;h2 id=&quot;碰碰运气&quot;&gt;碰碰运气&lt;/h2&gt;
&lt;p&gt;反正线上的core文件都已经生成了，不玩白不玩。通知运维把core给拿下来，我非得去看看到底为啥就crash了？不看不要紧，一看就自己打了自己的脸啊！&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;加载core文件，发现程序是因为收到信号6的原因进程被退出的。如下图：
&lt;img src=&quot;1.png&quot; alt=&quot;single 6&quot; /&gt;
信号6，也就是SIGABRT，Linux的man解释：由调用abort函数生成的信号。引起这个信号的可能性有很多，所以这一点没啥用。&lt;/li&gt;
  &lt;li&gt;加载可执行文件，然后看看crash的一瞬间的堆栈是一个什么情况，好做一个大概的判断：哪一行代码可能出现了问题。
&lt;img src=&quot;2.png&quot; alt=&quot;load file&quot; /&gt;
执行bt命令后，显示出来的stack信息全是？？，出现这种情况的可能性有2个：
    &lt;ul&gt;
      &lt;li&gt;可执行被重新编译了，导致了新的可执行文件的元数据和core文件中的元数据对不上，这个首先被排除，我们的可执行文件都是从线上直接拖下来的，所以不可能出现这种情况（这里也要告诫大家，千万要保留运行时程序的可执行文件版本，包括.o文件等）；&lt;/li&gt;
      &lt;li&gt;stack已经被破坏了。这种情况很容易就会发生。程序在运行的过程中因为stack，array等溢出的问题没有第一时间被crash，接着在执行命令的时候，产生的core文件stack可能就会被破坏；&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;目前来看，我们所能拿到的信息都已经指向了线索中断。操作系统给出的信号不能定位问题，使用gdb调试core因为stack被破坏无法再进一步。那么还有没有别的办法呢？&lt;/p&gt;

&lt;h2 id=&quot;再进一步&quot;&gt;再进一步&lt;/h2&gt;
&lt;p&gt;竟然stack已经被破坏，gdb也无法认出，按照正常的路子是解决不了这个问题了。所以这时候“野路子”（其实是更合理更深层次的解决方案）就上场了。既然被破坏，那就恢复它或者是想办法绕过它。&lt;br /&gt;
恢复stack的难度有点大，你首先得知道core文件所有的元数据信息，然后和可执行文件的元数据信息一一合并，还要考虑程序在runtime状态下的内存状态，难度确实是相当的大。所以这一步首先被排除了。&lt;br /&gt;
那就绕过它。考验计算机操作系统原理和计算机运行原理的时刻来了（所以要多读书）。&lt;br /&gt;
首先我们知道所有的程序都是由cpu来执行机器指令的，和cpu执行指令相互配合的是寄存器，其中有几个寄存器记录了当前程序runtime状态下的地址，比如esp/rsp，ebp/rbp等寄存器。也就是说我如果知道程序crash的时候寄存器的值，可能就有希望能复原当时的stack。
方针已经制定，就看执行了。&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;首先看一下各个寄存器的值，如下图:
&lt;img src=&quot;3.png&quot; alt=&quot;regediter&quot; /&gt;
别的都不用看，只要看一下rbp和rsp的值就可以了。
    &lt;ul&gt;
      &lt;li&gt;rsp表示是当时程序runtime的时候栈顶的地址值；&lt;/li&gt;
      &lt;li&gt;rbp表示当时程序执行到的指令的地址值；&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;再看一下执行到rbp的指令的时候，程序前面都执行了什么指令，如下图：
&lt;img src=&quot;4.png&quot; alt=&quot;rbp&quot; /&gt;
哈哈，终于看到函数了。spx_socket_connect_nb和ydb_storage_heartbeat_send。按照stack的FILO的原理，是执行到spx_socket_connect_nb的时候程序发生了问题。具体的地址在函数的地址+611处。先看一下汇编，查一下地址看看能不能看出来一点什么？如图：
&lt;img src=&quot;5.png&quot; alt=&quot;code&quot; /&gt;
好像没啥，前面就是给connect构造结构体，后面就是调用connect。但是问题确实是在connect之前就crash了啊。查看一下代码:&lt;/p&gt;

    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; err_t spx_socket_connect_nb(int fd,string_t ip,int port,u32_t timeout){
     struct sockaddr_in addr;
     bzero(&amp;amp;addr,sizeof(addr));
     addr.sin_family = AF_INET;
     addr.sin_port=htons(port);
     addr.sin_addr.s_addr = inet_addr(ip);
     err_t err = 0;
     err_t rc = 0;
     if(0 &amp;gt; connect(fd,(struct sockaddr *) &amp;amp;addr,sizeof(addr))){
         //filter this errno,
         //socket is not connect to server and return at once
         if (EINPROGRESS == errno) {
             struct timeval tv;
             SpxZero(tv);
             tv.tv_sec = timeout;
             tv.tv_usec = 0;
             fd_set frd;
             FD_ZERO(&amp;amp;frd);
             FD_SET(fd,&amp;amp;frd);
             socklen_t len = sizeof(err);
             if (0 &amp;lt; (rc = select (fd+1 , NULL,&amp;amp;frd,NULL,&amp;amp;tv))) {
                 if(0 &amp;gt; getsockopt(fd,SOL_SOCKET,SO_ERROR,(void*)(&amp;amp;err),&amp;amp;len)) {
                     err = errno;
                     return err;
                 }
                 if (0 != err) {
                     return err;
                 }
             } else if(0 == rc) {
                 err = ETIMEDOUT;
                 return err;
             } else {
                 err = EXDEV;
                 return err;
             }
             SpxErrReset;
             return 0;
         } else {
             return errno;
         }
     }
     return 0;
 }
&lt;/code&gt;&lt;/pre&gt;
    &lt;/div&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;确实前面我们就构建了一个struct sockaddr_in的结构体，然后我们就直接connect了。貌似不会出问题，而且代码也已经运行了很久很久了。但是突然间，有种预感冥冥中就出来了。nm可能就这里出问题了。去看一下配置文件，其中的配置项：
&lt;code class=&quot;highlighter-rouge&quot;&gt;stacksize = 64kb&lt;/code&gt;
直刺眼睛啊！火辣辣的。&lt;br /&gt;
这个配置项用来做什么的？其实这个配置项是用来限制系统的stack大小的。也许很多同学都没有听过东东，但是如果你在linux上运行一下命令：
&lt;code class=&quot;highlighter-rouge&quot;&gt;ulimit -a&lt;/code&gt;
&lt;img src=&quot;6.png&quot; alt=&quot;ulimit -a&quot; /&gt;
会出来如上图的一排设置，其中红色框框圈出来的就是这个stacksize的值。在这台机器上默认的是8mb。stacksize的设置在每种linux发行版上的值可能都是不同的。当初就是为了“装逼”，显示自己的编程水平，将stacksize设置成了64kb，这样程序中每个线程最大的stack可用大小就是64kb，你看看我控制系统资源控制的多好？！结果，当碰上一不小心不注意的时候，stack的size马上就超出了64kb，这样stack就溢出了。当然程序就会crash，stack也当然的被破坏了。&lt;br /&gt;
那么为什么我们的会出现这个问题呢？&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;sockaddr_in的结构体其实并不是很大，但是它申请的是stack的内存，原来使用的内存大小+sockaddr_in的大小应该正好碰上临界点；所以sockaddr_in的申请应该是最后一根稻草；&lt;/li&gt;
  &lt;li&gt;ydb_storage_heartbeat_send这个函数是在heartbeat线程中运行的，heartbeat线程是一个常驻线程，由timer事件来触发，每次都会向tracker发送心跳数据，所以可能会存在stack因为某些原因，导致了再一次运行中申请的内存比较多，比如log记日志的时候；&lt;/li&gt;
  &lt;li&gt;那么为什么不起来就出现这个问题呢，而是要在运行了一段时间后呢？这个问题其实没办法确定申请stack的时间点，比如因为网络的问题我需要记录日志，然而又不是时时刻刻网络不行的，正好在某个瞬间网络不行，申请stack内存记录日志，这样没有释放正好被抓到；&lt;/li&gt;
  &lt;li&gt;又因为heartbeat是常驻线程，所以stack基本上不太会被第一时间回收，肯定要在事件处理的最后被回收，所以在一次事件处理内一定要确保有适当的大小来满足程序对于stack的需求；&lt;br /&gt;
原因也找到了，怎么解决这个问题呢？有几种办法：&lt;/li&gt;
  &lt;li&gt;不要装逼了，直接这个项不用设置。我们的程序使用的是事件模型，并不是像java一样的每个connection一个处理线程，dfs是一个线程对应着多个connection，所以线程数并不多，就算是每个线程最大的stacksize是8mb，也用不了多少的内存；&lt;/li&gt;
  &lt;li&gt;把配置文件中的stacksize改大一点，比如1024kb什么的就可以了。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;经验教训&quot;&gt;经验教训&lt;/h2&gt;
&lt;p&gt;因为stacksize这个项的设置导致了出问题的人我不是第一个，也不会是最后一个，仅仅在dfs出了这个问题的时候，我就知道还有别人因为同样的问题导致程序被crash，只是当时程序跑的好好的我就没有多加注意。所以在这里诚恳的告诫大家：&lt;br /&gt;
莫装逼，装逼遭雷劈;&lt;br /&gt;
莫装帅，装帅遭人踹;&lt;br /&gt;
莫装吊，装吊遭狗咬…&lt;/p&gt;
</description>
        <pubDate>Tue, 27 Jun 2017 00:00:00 +0800</pubDate>
        <link>http://www.94geek.com/blog/2017/gdb-stacksize/</link>
        <guid isPermaLink="true">http://www.94geek.com/blog/2017/gdb-stacksize/</guid>
      </item>
    
      <item>
        <title>如何更好的做一堂技术topic分享的套路</title>
        <description>&lt;p&gt;很荣幸的被csdn选中担任sdcc2017深圳架构场的出品人。更荣幸的是，我推荐团队一个小伙伴的技术topic也被选中，将在sdcc20017深圳场和大家分享。但这位小伙伴是第一次作为讲师参加技术topic这样的活动，所以培训小伙伴的事情就成了当务之急。作为一个经常出入技术topic、已经无所谓脸皮不脸皮的过来人，对做一堂还算相对“和谐与靠谱”的技术topic到底有多少的套路呢？&lt;/p&gt;

&lt;p&gt;总的来说， 一堂技术topic的套路分为几个部分：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;PPT&lt;/li&gt;
  &lt;li&gt;前期准备&lt;/li&gt;
  &lt;li&gt;分享中&lt;/li&gt;
  &lt;li&gt;结束&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;ppt&quot;&gt;PPT&lt;/h2&gt;
&lt;p&gt;PPT永远是一马当先的部分！ppt在整个的topic中占的比重应该是最大的。原因有几个：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;前期主办方唯一需要并且讲师唯一能证明自己的东西；&lt;/li&gt;
  &lt;li&gt;你所要表达的思想和内容都在这张PPT中；&lt;/li&gt;
  &lt;li&gt;在分享中，PPT也是你主要依赖的对象，是你控制整个topic的主线轴；&lt;/li&gt;
  &lt;li&gt;ppt会被主办方以各种形式下发，以确保影响力；&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;所以我们需要更加重视的对待ppt。按照我目前参加技术topic的讲师经验和这次的出品人经验，总结讲师在做PPT时候注意点有以下几个：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;字体。ppt中的字体要尽量的大。有的同学会问我：具体要多大？能不能给一个明确的号数？答案是然后并没有。但如果可以的话，把字体设置成所能设置的最大号。大到你觉得这页PPT不好看了，就像“老人手机屏幕”一样了，基本上就可以了。也有很多同学不理解，为什么好好的一页PPT字体要那么大呢？作为讲师你必须要考虑现场，ppt的字体小了，topic现场后面的同学看不见。而讲师在做PPT的时候通常是没察觉的，因为讲师做完PPT预览的时候都是在自己的电脑屏幕上进行的，所以并没有现实的距离感。&lt;/li&gt;
  &lt;li&gt;色彩。这里推荐的色彩是“暗色系的底、亮色系的字”。这样的设定可以最大程度的提高对比度，以帮助现场的同学尽可能清楚的看清楚ppt上的内容。在制作ppt的过程中，尽量不要使用中色系的颜色，也不要使用晃眼的亮色系，比如黄色等等颜色，使用晃眼的亮色系颜色在现场再加上投影或者是LED的光源，不仅仅是看不清的问题，而是眼睛会很疼。&lt;/li&gt;
  &lt;li&gt;背景。一般主办方会给你一个他们设定的母版页来作为你整个ppt的基调。但是请大家注意，如果对方给的母版页的背景是很花里胡哨的、或者是背景图画部分特别多（占到整张PPT的25%以上的），这样的就需要讲师自己权衡一下了。要么你自己做母版页自己设定背景，要么就不要背景，直接白色。其实依照我的经验，你拿到的母版页不和你口味的比率占到99%以上。所以在可能得情况下，尽量的自己做背景，而对于技术的topic，强烈推荐纯白色背景。&lt;/li&gt;
  &lt;li&gt;分辨率。目前国内的投影器或者是led屏幕，大部分都是4:3的分辨率，而我们的电脑基本上都是16:9，所以请各位在制作ppt的时候一定要事先和主办方或者是出品人沟通好，现场的ppt屏幕比例到底是多少，然后再开始你的ppt。特别是对于有特效的同学，当你的显示比例不对的时候，动画的效果会很差很差。&lt;/li&gt;
  &lt;li&gt;内容。内容主要分以下几块：
    &lt;ul&gt;
      &lt;li&gt;个人介绍. 个人介绍一般都是必不可少的。毕竟你又不是什么人尽皆知的大人物，在开场的时候做个个人介绍还是很有必要的，一来可以让大家先认识一下你，知道你在哪里做什么工作；二来如果你可以在这个阶段加入一些你公司的产品介绍就更好了，在增加人家对你印象的同时顺便给公司做了一个“软广”。&lt;/li&gt;
      &lt;li&gt;目录。这部分很简单，一般也是必不可少的。&lt;/li&gt;
      &lt;li&gt;正文。正文需要说几个注意点：
        &lt;ul&gt;
          &lt;li&gt;字。在ppt中尽量避免一大段一大段文字的情况。这种大段大段的文字基本上没人看。如果要出现这种文字，首先考虑是不是可以图形化，其次考虑是不是可以用list或者table的方式来表达；&lt;/li&gt;
          &lt;li&gt;图。图除了色彩搭配和上面提到的一样需要注意外还需要注意2个问题。图尽量的大，并且尽量的不要失真。第二，尽量不要使用截图，特别是网页截图。这种截图往往很难拉伸，ppt的展示效果比较差；&lt;/li&gt;
          &lt;li&gt;动态效果。在技术ppt中，动态效果不宜过多（PS：我指的是一页中），一般除非需要表示数据流等动态流程图，否则应该尽量少的使用动态效果。动态效果用的好是画龙点睛，用的不好就是画蛇添足。而对于技术性的讲师来说，往往是后者。所以宁愿不用也不要错用；&lt;/li&gt;
          &lt;li&gt;广告。这是一个非常严肃又非常值得讨论的问题。大多数的讲师都会在ppt中加入广告，99%的讲师会加入招聘广告，虽然大家都知道通过这种方式成功的概率微乎其微，但还是在做。正是印证了一句广告界的名言：99%的广告打下去都是没结果的。但是为什么大家都在打呢？因为不知道那1%是在哪里！这里要提醒各位的是，做广告可以，但是吃相不要太难看。比如在topic上写上别人家的活动报名方式什么的，这种就有点夸张了。至少也要尊重一下本次的主办方吗；第二，广告尽量使用“软广”的方式，那种硬性植入就能免则免，太欠缺水平了，也没有情怀和b格；第三，大家可以把Q&amp;amp;A页利用起来，上面放一个公司或者你自己的开源github二维码、公众号二维码之类的图片，让大家觉得你不是在做广告，而是在增加联系。归根结底一句话：广告要做的有水平，不要让人家一眼就看出来你这是在做广告，要有b格和情怀，要有b格和情怀，要有b格和情怀！重要的事情说3遍。&lt;/li&gt;
          &lt;li&gt;页数。 一场topic大概用时一般在45分钟，很少有1个小时的，也很少有低于45分钟的。所以对于讲师来说，你真正拥有的时间大概在30-40分钟之间。那也就是意味着你的ppt页数不能太多，也不能太少。基本上控制在有效页数在15-18页，最多不超过20页、不要少于12页就可以了。虽然我也见过50页ppt半小时讲完的，或者是3页ppt讲2个小时的，但是这种人毕竟是少数。我自认为你不是这种人。有效ppt，是指你真正讲内容的ppt。中间有一些穿插的标题、不展开的结论、ppt转换等这些都属于一带而过的，所以它们并不是有效ppt。那么除了这些，剩下的的ppt，包括了topic大部分真正内容的页，这些都是有效ppt。&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;pdf。一般做完ppt后也要同时提供一份pdf给主办方，作为主办方现场或者是会后发放的内容。这份pdf大家一定要注意，一定要脱敏，特别是公司的一些数据，个人的信息等等。一定要进行脱敏处理后再发送给主办方。&lt;/li&gt;
  &lt;li&gt;交稿时间。一般都是认为越早越好，但我觉得不是。最好的时间应该是开始前2个星期开始做ppt，花几天构思，然后花1天完成。这样可以安排在最后期限的前一个星期这个时间点上提交你的ppt。这个时间点的好处是，首先你至少不会到了现场忘记了ppt，也不会因为你要参加topic影响你的工作；第二，你也需要给出品人一点时间来审核你ppt；第三，你还给了你自己一点时间来修改和完善你的ppt。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;前期准备&quot;&gt;前期准备&lt;/h2&gt;
&lt;p&gt;等交稿一个星期后，基本上你的技术topic分享也要开始了，在分享的过程中，也需要几个注意点：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;到达时间。建议大家尽量提前达到topic举办的城市。一般也不用提前太多，前一天晚上到，第二天分享就可以了。通常主办发会在topic举办的酒店或者就近酒店给你开房间，所以你去了以后尽可能的先去现场转转，特别是你要分享的分会场现场，去看看大体的布局、投影的效果、讲台的位置等等。这样至少能做到前期心中有数。&lt;/li&gt;
  &lt;li&gt;设备问题。建议大家最好能自己带上笔记本，如果有条件的话可以自己带好演讲笔。这个不是必要条件，但以防万一。特别是技术同学，现在很多技术同学使用的都是mac，经常提交的是keynote，而国内目前windows还是非常的通用，所以现场可能会出现ppt放不出来或者ppt有问题等等。不过就算你用的windowns+ppt，有的时候也会因为系统或者ppt的版本不同而发生问题，所以最好还是带好自己的笔记本，出现问题的时候可以立即就解决。&lt;/li&gt;
  &lt;li&gt;最后一眼。在分享的前一天晚上，或者你的topic安排在下午的话就是当天上午，尽量抽时间最后看几眼ppt，查漏补缺一下。这是在分享前最后更新ppt和分享知识的机会，也可以再一次熟悉一下自己要讲的ppt和要讲的内容。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;分享中&quot;&gt;分享中&lt;/h2&gt;
&lt;p&gt;就要来到分享现场了，大家是不是有点紧张？这里就来讲讲分享现场需要注意的问题。&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;紧张。首当其冲的就是紧张问题。不管你是新手还是“老司机”，其实都会紧张的。特别是马上要轮到你了，还没轮到的时候，心理都会有一些波动。这种紧张或者叫担心是很正常的。老司机可能在开始的时候就变的正常了，而新手还是会紧张。这里想和大家说的是，不要太担心，经过上面的准备后，这个环节应该不会有什么大问题。你站在讲台上，相比下面的听众，你有更多的天然优势。你分享的内容你是最熟悉的，下面的听众都没有你熟悉，所以你只要按照你前期的准备按部就班的下去就可以了。&lt;/li&gt;
  &lt;li&gt;怎么讲。这里也需要分几个部分来讲：
    &lt;ul&gt;
      &lt;li&gt;脱稿。首先是要尽量的把ppt串起来，并且在分享的过程中除了需要使用演讲笔现场指出来ppt上的部分内容外，总体上需要脱稿。这也是为什么我在上面强调的：不要太早的做完ppt的原因之一。在整个分享过程中要注意节与节之间的衔接部分，这里特别容易“卡壳”。&lt;/li&gt;
      &lt;li&gt;单页讲解。单页讲解的时候最怕的就是口语。比如很多同学喜欢用“然后”，“嗯”，“下面”，“接下来”……这种的口语化的表达方式，而且是重叠的出现这种，比如一页ppt中不停的“然后”…“然后”…“然后”…。这是很不可取的。一般一页ppt总归有个前后关系，所以最好的方式是多使用层次性的词语来表达，比如“首先”…“然后”…“其次”…“接着”…“最后”…,或者是“第一”…“第二”…“第三”…。&lt;/li&gt;
      &lt;li&gt;肢体语言。在分享的时候，一般一手拿着麦克风，一手拿着演讲笔，幸好人类没有第三只手，否则更乱了。所以很多同学都是拿着麦克风的那只手举着（没办法，至少要说话），而拿着演讲笔的那只手不知道在干嘛。有的垂的，有的乱晃，还有的直接插兜里（PS：虽然我也觉得插兜里挺帅挺个性）……，这些希望大家都不要出现。拿着演讲笔的那只手可以适当的做一些动作，不要太夸张也不要一动不动。适当的做一些动作可以让大家觉得你更融入此次的分享中，也可以让大家更加接受你分享的内容。至于这个度，需要自己把握，确实挺难。&lt;/li&gt;
      &lt;li&gt;掌控讲台。大家千万不要在讲台上一动不动的站着，从上去到下来都在一个地方；或者是直接站在演讲台那边，看着ppt在那里好像在做政府工作报告。你可以适当的在讲台上走走，你要清楚你是讲师，这个讲台在这个45分钟内是属于你一个人的，何必那么拘束呢？一般技术topic的讲台都是一个高于听众的地方，长方形，也不是特别的大。但是绝对在中间。你在讲台上适当的活动一下相比站在一个地方不动，可以照顾到下面更多的同学。可以让大家更好的融入到自己的分享中。&lt;/li&gt;
      &lt;li&gt;提问互动。关于这点公说公有理婆说婆有理的问题确实不好办。但依据我的经验，在分享的过程中尽量避免和下面的听众进行沟通和对他们提问。尽量的少问“这个问题大家知道吗？”，“这个有谁知道吗？”这种问题。首先，你对下面听众的情况不熟悉，你这么“唐突”的提问很少能拿到你想要的答案；其次，这种提问几乎都会冷场，很多最后都是讲师自己尴尬的打圆场过去，很少有人会举手回答，就算有也是你要多次的确认“有人知道吗？”；第三，这是技术分享，不是培训。这点要分开。公司内部的培训可以经常使用这种提问方式，但是演讲不太适合，培训的人数一般只有十几个甚至最多也就几十个，如果人数多了就会分批，规模上好控制，而演讲这种的一般至少100-200人吧，会场也很难控制。&lt;/li&gt;
      &lt;li&gt;段子。不要笑，这是一个很严肃的问题。特别是对于在中间分享的讲师，一定要准备3-4个段子。一般大家在长时间的集中精神后，中间有段时间特别累。其实我们自己也有感觉，你坐在下面听一下试试看，一个上午就受不了了，比上班还累。所以在分享的过程中需要穿插适当的段子来把听众的小差给开回来。当然了也不排除很多同学就是为了听你的段子来的。老罗曾经说过：我讲的题你们都没记住，我讲的段子你们倒是记得清清楚楚，过后还久久回味、栩栩如生。&lt;/li&gt;
      &lt;li&gt;时间控制。这个主持人一般都会帮你。有做的比较好的，比如csdn，会给你一个大大的ipad，给你看从开始到结束的剩余时间倒计时；没有这种ipad，也会有一个最后5分钟，最后3分钟，最后一分钟这样的倒计时提醒牌。有倒计时和没倒计时是一把双刃剑。有吧，总感觉有人在催你；没有吧，总会超时。所以归根结底这还是需要大家多加练习自己的ppt，学会控制时间。这里要说的是尽管很多时候最后肯定会拖时间，但是大家要注意，千万不要拖太多就可以了，拖一个2-3分钟没人会说你，因为主持人会灵活掌握下面同学提问的时间，但有一次我也是讲师，我前面的一个哥们拖了15分钟之久，这就太夸张了。而且TMD拖时间是因为他要做广告，这就让我很无语了。所以我对这个哥们印象很不好，大家都在这个圈子混，名字我就不说了。&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;出现意外。这很正常，比如下面突然有人叫起来：这玩意那里听过，或者是这个ppt我看不清楚。出现这种情况首先是千万不要紧张，也不要不知所措。你可以先安抚一下听众，第二，赶快和主持人沟通怎么解决问题，是不是能把ppt的pdf文档下发下去之类的。这里要说一下，如果你的ppt因为公司保密原因不能下发，那么请各位讲师要和主办方、主持人事先沟通好，商量好万一出现问题的时候怎么解决。这种事情不是没有发生过，我就见过因为这个事情现场闹得不可收拾的。这样对大家都不好，特别是对讲师所在的公司名声影响很差，还不如不要出来做这次分享呢！&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;结束&quot;&gt;结束&lt;/h2&gt;
&lt;p&gt;一般分享结束后，剩下的就是听众提问和讲师解答的阶段了。到这里其实才是最考验各位讲师内功的，因为提问环节基本上你不太可能能全部掌握。就算你前面准备的再好，这部分可能最多你也就只能覆盖到70%。那么这部分会经常出现哪些问题呢？&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;问题超纲。这里的超纲是指超出了讲师的知识所掌握的范围。对于初次作为讲师的同学可能是非常容易碰到的。其实这个并不可怕。解决这个问题的本质还是要奉劝大家平时多加注意积累，多加注意同类型开源软件或者是同类型开源论文等东西，只是临时抱佛脚是解决不了这个问题的。但如果你已经站到讲台上了，碰上这种问题你再告诉是靠积累，你这确定你不是在开玩笑？所以一种办法是你确实回答不上来，那么就大方的承认这个你没考虑过，后面再看看；另外一种可能的情况是你其实知道一些，但是知道的知识并不是特别的清楚，没有十足的把握也没有那么的全。这种情况下你就找几个你有把握的，可以回答这个问题中的几个点就可以了。一般主持人看见这种情况也会帮你解围的。&lt;/li&gt;
  &lt;li&gt;一直追问。 这个是对于下面的听众来说的。如果某个哥们一直霸占着话筒怎么办？这个任务一般是主持人的。他会解决这个问题。我的经验是一般一个人提问一次后我就收走话筒了，一来可以给另外的同学更多的提问机会；二来，还可以兼得保护讲师。但是如果主持人也无动于衷，那你就可能需要自己解决这个问题。在回答完他的问题后，你可以带一句“这位同学，下去了我们微信交流，希望现场能有更多的同学提问”这样的话来结束这个同学的提问。&lt;/li&gt;
  &lt;li&gt;一人多问。这个也很正常。一般一个人的记忆力是有限的，所以你可以一个一个的来回答他的问题。但是一般首先强调一下一次提问最好不要超过2个，最多不要超过3个，因为实在是记不住。如果问完了还是有问题那就私底下微信交流。&lt;/li&gt;
  &lt;li&gt;现场群。多看一下群，万一有同学问到你，如果你不回答的话就显得不太好，所以还是要照顾一下群里的反应。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;
&lt;p&gt;套路，都是套路。写书有写书的套路，程序有程序的套路，分享也有分享的套路。大家都是按照套路在办事情。目前根据我的经验，基本上作为一个讲师，所能想到的套路就那么多。但是套路归套路，归根结底，还是希望大家如果有志想挑战一下自己，有一天也想站上讲台的话，在套路的同时也一定要对于分享这件事情多加练习。其实每个讲师虽然会使用套路，但是私底下还是很努力练习的。这次我们上sdcc的小朋友在事前被练了一个多星期，用他的话说被练的自己都不知道怎么了。ppt在公司内部走了有4-5遍。先是我们几个内部听，提出问题、整改；然后是部门内部听，提出问题、整改；然后再是我们内部听，提出问题、再整改，以此反复。所有的好分享除了套路更多的还是建立在多加练习、熟能生巧、以勤补挫的基础上的。大家千万不要以为会了套路就会打拳了，这仅仅只是招式，内功还是要靠自己一点一点，日积月累而成的。&lt;/p&gt;

</description>
        <pubDate>Thu, 15 Jun 2017 00:00:00 +0800</pubDate>
        <link>http://www.94geek.com/blog/2017/how-do-topic/</link>
        <guid isPermaLink="true">http://www.94geek.com/blog/2017/how-do-topic/</guid>
      </item>
    
      <item>
        <title>SDCC2017 深圳架构场前瞻</title>
        <description>&lt;p&gt;首先很荣幸的被csdn邀请为sdcc2017深圳架构场的出品人参与此次topic。作为出品人，在演讲嘉宾的筛选上就已经动了脑筋。此次sdcc2017，演讲嘉宾的阵容是非常强大的，既有腾讯、阿里、百度传统3强的分享，又有唯品会、阅文集团、sina微博等公司的强势加入。各位演讲嘉宾也都是业界的“老司机”，不管是从技术还是演讲本身来说，都是“老”的可以啊！所以sdcc2017深圳架构场的技术topic水平当然不会低，是绝对值得期待的。&lt;/p&gt;

&lt;p&gt;当出品人还有一个特权就是可以提前看到各位演讲嘉宾的PPT。此次架构topic的PPT在各位“老司机”动不动就“一言不合”加大马力“飙车”的情况下各个卓越超群，各显神通。这里我就先瞒过csdn的小编来报个料。&lt;/p&gt;

&lt;h2 id=&quot;上午&quot;&gt;上午&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;首先登场是来自阿里的中间件架构师冯嘉，他为大家带来了来自阿里中间件团队自我研发的消息队列中间件“RocketMQ”。大家都知道阿里面临的访问、流量、正确性等一系列的压力；也明白在这些压力下程序员所要面对和承担的技术困境。RocketMQ在经过阿里内部多年的开发和使用后，顶住了压力（特别是“丧（疯）心（狂）病（数）狂（钱）”的双十一）。同时也因为出色的性能表现和卓越的稳定性，RocketMQ加入了Apache基金会。此次topic，冯嘉主要为大家带来了RocketMQ的什么呢？可以用下面几点来概括：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;首先是阿里使用RocketMQ的历程，从使用开源软件到自我研发RocketMQ的煎熬和解脱；&lt;/li&gt;
  &lt;li&gt;RocketMQ的实现。此次的topic可以说是毫无保留的全部拿出来了。包括mq的内部存储结构图都一清二楚的画出来贡献给大家看了，真是印证了那句话：是骡子是马拉出来溜溜；&lt;/li&gt;
  &lt;li&gt;接着可能是大家最关心的RocketMQ具体的特性和性能。特性和性能可以让大家在选型和使用的时候更加的安心；&lt;/li&gt;
  &lt;li&gt;最后是RocketMQ的监控与后续发展。这部分可以确保大家更加方便的使用RocketMQ；&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;紧接着登场的就是来自新浪微博的技术专家陈波，他为大家带来了“微博Feed的缓存架构及其演进之路”。微博的用户量亿级也是妥妥的、pv，并发更是不用讲了，大家心里都有数。所以他们在组织feed的确实也碰到了很大的困难和很多的现实问题。此次topic，陈波主要为大家带来了feed缓存的3个主要方面：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;首先是feed平台的总体架构，说明了feed在微博整体架构中所占的重要位置和实际碰到的问题；&lt;/li&gt;
  &lt;li&gt;解决feed所使用的cached的架构极其实现；&lt;/li&gt;
  &lt;li&gt;接下来是此次topic的亮点，对redis更好使用的案例。这部分应该会有很多人想一探究竟，毕竟redis几乎已经成为高性能网站的“威尔刚级” 产品了，现在还有谁敢说不用redis？&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;上午的最后一场topic来自于腾讯的架构平台部高级工程师陈杰，他为大家带来的是“基于空闲资源的弹性计算实践
”。弹性计算，虽然一直在说，但是对于目前的行情来说其实还算是一个比较新的领域，或者说是对普遍的大家一个比较陌生的领域。这是因为很多公司根本就没有这个精力也没有这个实力更是没有这个必要和动力去研究这个东西，但是它却是大企业或者说是云计算必须要面对的一个问题。如何更好的去利用空闲的机器已经是很多像腾讯、阿里等一样规模的大型互联网公司不得不也是必须要去面对和解决的一个问题。这次topic陈杰给大家主要从以下5个方面来讲解弹性计算：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;首先是弹性计算在企鹅内部的需求。从企鹅的实际需求出发，可以让各位清楚的知道什么情况下或者说什么状态下弹性计算是必要的；&lt;/li&gt;
  &lt;li&gt;实现弹性计算的难点。这应该是业界的一个难题，可以看看企鹅是怎么去解决这个问题的；&lt;/li&gt;
  &lt;li&gt;“4步法”解决企鹅弹性计算在线业务的质量保障。你问我哪四步？我先卖个关子；&lt;/li&gt;
  &lt;li&gt;“3步法”解决企鹅弹性计算怎么使用好。又要问我哪三步？我是想说的，但是被csdn的小编给删除了！&lt;/li&gt;
  &lt;li&gt;最后是企鹅弹性计算团队在实施弹性计算的过程中碰到的问题和其解决办法。这是企鹅弹性计算团队的知识结晶，大家不容错过啊！&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;大家可以先吃点饭，休息一下！下午更精彩哦！&lt;/p&gt;

&lt;h2 id=&quot;下午&quot;&gt;下午&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;下午首先登场的是来自唯品会的架构师薛珂，他为大家带来了唯品会内部使用的“弹性调度平台Saturn的架构设计”。Saturn说白了就是一个Job系统，但是Saturn相比其它的Job系统有它独到的地方，而且也有更多的特性，主要的亮点在于以下4个方面：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;job的无语言限制支持。可以支持目前常见的各种语言，甚至是shell脚本也可以无缝的运行；&lt;/li&gt;
  &lt;li&gt;触发机制支持事件和时间。目前市面上更多的Job基本上都只支持时间触发机制，最复杂也就是用Cron表达式，所以Saturn支持事件是一个亮点；&lt;/li&gt;
  &lt;li&gt;支持任务分片和根据机器负荷动态负载平衡。支持分片的Job很多，但是根据Job执行机器的系统运行指标来动态调整job执行的负载均衡是一个不错的想法和实现，也是Saturn的一大的亮点；&lt;br /&gt;
4.支持云部署。云是目前最火的一种架构模式，Saturn也不例外的支持了。从这点上也可以看出Saturn是走在技术架构时尚前沿的弄潮儿啊。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;虽然这时候大家可能有点困了，但是千万不要瞌睡，错过了好戏我可不负责！&lt;/p&gt;

&lt;p&gt;下面出场的是来自百度网页搜索部的资深研发工程师陶清乾，他为大家带来的是前端架构：“基于PWA的Web App前端架构探索与实践”。这是此次深圳架构场唯一一场前端架构topic的分享。他主要为大家从以下3个方面讲解百度在实际的工作中使用PWA架构移动Web App的经验：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;目前移动端web碰到的困境和百度所使用的解决方案；&lt;/li&gt;
  &lt;li&gt;PWA技术的价值与在百度使用过程中的效果。这部分大家应该可以从现在百度的部分app中就能感受出来吧？不过亲身听一下设计人员的讲解可能会更加的事半功倍！&lt;/li&gt;
  &lt;li&gt;前端架构新思路。从离线化、视图框架、异步渲染和交换感4个方面来说明前端架构设计的新思路；&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;第三场来自于前卷皮网架构部技术总监陈秋余，他为大家带来的是“领域模型 + 内存计算 + 微服务的协奏曲：乾坤”。这场也是此次唯一一场分享目前比较流行的“微服务” 的技术topic。领域模型是一个架构中经常会碰到的问题，也是几乎目前的系统中主要的组成部分；内存计算大家也都耳熟能详；至于微服务嘛，最近火的不要不要的。那么这3个精兵强将组合在一起能碰撞出多少的火花呢？卷皮网给出的答案就是“乾坤”系统。这场topic将从4个方面对于“乾坤的火花”进行阐述：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;特卖类电商的微服务架构。这是很多传统电商都不会碰到的问题，偶有幸（也是不幸）碰到过。很早之前在5173任职，5173虽然不属于一个传统电商网站甚至都不属于特卖类网站，但是游戏交易，几乎有和特卖类电商有着一模一样的业务和架构痛点。其痛点就两个字：“比快”。不管是特卖还是抢购，其实比的就是一样：顾客“手快”的情况下网站不down机并且快速且正确的响应！&lt;/li&gt;
  &lt;li&gt;目前主流分布式架构及其问题。这部分主要分析了特卖类网站和目前主流分布式网站架构的矛盾之处，笔者看过后满满的“惺（泪）惺（流）相（满）惜（面）”；&lt;/li&gt;
  &lt;li&gt;解决高并发争抢的性能问题：无锁排队。这种解决方案现在越来越多的被提及。我们自己开发的很多中间件也都是使用了类似的解决方案来处理这个问题。到底怎么处理？还是来听听吧！说多了你就不买票了！（这不是偶的本意，csdn小编改的，你们可以去找他）&lt;/li&gt;
  &lt;li&gt;分布式事务的一致性模型。其实大家从开始分布式架构系统开始就碰到了事务这个难题。虽然时间很长久，但是一直到现在为止，都没有一个统一的、标准的实现方案方法。基本上都是各家根据各家的实际业务模型和情况再结合2PC、3PC或者及其变种来实现解决。当然乾坤也不例外了；&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;最后一场是来自阅文集团。这里不得不多夸奖几句阅文集团。阅文集团这几年越来越多的参与各种技术开源和技术分享，阅文集团的技术也正在慢慢变的成熟。这里也希望阅文集团能够越来越多的帮助到各位在技术上碰到的问题。（哈哈！这个不得不说，是偶“不要脸”了。的确是“硬广”，纯粹不要脸的那种“铁硬广”）。PS：小编别删了啊，看在偶半夜写文档、也没有大尺度的上二维码的份上，留点面子。另:我们开源的二维码是…&lt;/p&gt;

&lt;p&gt;言归正传，最后是来自阅文集团的架构师帅翔为大家带来的“PB级去内存化分布式缓存系统Lest的架构设计与实践”。对于帅翔偶还是比较了解的，因为天天被偶“虐”。Lest项目也是我们目前的重点项目，它主要的作用就是用来在生产环境替换掉目前阅文集团内容中心存在的大量主动缓存redis。对的，你没听错，我们准备下架一部分的redis。其实对于redis，大家也都心知肚明：随着redis的大量使用，基本上最后就是失去控制。病理表现：redis服务器或者redis内的缓存内存只加不减。那这是为啥呢？因为里面有点啥，有的时候你真的很难搞得清楚，哪怕你是一手做起来的，并且管理起来的，久而久之就这样了。这基本上也是现在很多公司共同碰到的难题之一。另外一个问题，也是一个很大的问题：redis是基于内存的，虽说现在内存白菜价，但是毕竟还是要成本。当你大量使用的时候，成本还是挺大；而且基于内存的一down或者重启，基本上就废了，有多少公司因为redis出了问题导致了站点down掉？所以用武侠来形容redis呢，我认为就是“离别钩”。不用吧不行，用吧闹不好自伤。那么我们解决这些问题的办法就是Lest！此次帅翔给大家从以下几个方面来讲述lest的前因后果：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;首先是Lest研发的原因和其总体架构。原因前面唠叨过了，这里说一下架构。相比redis，Lest增加了一层tracker来做一个统一的cached分配算法和存储的管理。这样的好处除了可以更好的管理cached内容，还能可以让业务系统和具体的分配算法隔离开，可以让业务系统更加的灵活；&lt;/li&gt;
  &lt;li&gt;数据的存储。和上面阿里的RocketMQ一样，我们也是无所保留的贡献出来所有的设计思路和方案，包括具体的磁盘存储格式。其中Lest使用的二进制协议，来自于messagepack的“阅文DIY升级版hermas”更是亮点，它既可以用来做网络通讯，又可以满足磁盘存储，lest的成功实施很大程度上是hermas的功劳；&lt;/li&gt;
  &lt;li&gt;数据的同步和性能测试。lest相比于redis就是“在尽可能少损失性能的基础上多了最后一致性和持久化”。众所周知，同步和持久化肯定会是性能的杀手，那么这部分就是讲述了lest用了什么技术和方案来去解决这个问题；&lt;/li&gt;
  &lt;li&gt;最后是lest目前所存在优点与缺点。诚恳的分析lest的先进性和目前所存在的问题，以及以后可能的发展方向，为大家以后的选型扩展了思路。&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;

&lt;hr /&gt;

&lt;p&gt;原本小编说只要1k字就够了，结果轻描淡写的就妥妥2k+了。这是因为参加此次技术topic分享的各位“老司机”飙车实在太厉害，偶也跟着“鸡血”了一把！对于这么多位高手云集的sdcc2017深圳架构场，你是不是已经手抖的都握不住鼠标拼命要去点击“购票”了呢？我们现场见吧！&lt;/p&gt;

</description>
        <pubDate>Wed, 07 Jun 2017 00:00:00 +0800</pubDate>
        <link>http://www.94geek.com/blog/2017/sdcc2017_%E6%B7%B1%E5%9C%B3%E6%9E%B6%E6%9E%84%E5%9C%BA%E5%89%8D%E7%9E%BB/</link>
        <guid isPermaLink="true">http://www.94geek.com/blog/2017/sdcc2017_%E6%B7%B1%E5%9C%B3%E6%9E%B6%E6%9E%84%E5%9C%BA%E5%89%8D%E7%9E%BB/</guid>
      </item>
    
      <item>
        <title>Chaos-ID生成器的前世今生</title>
        <description>&lt;p&gt;从mongodb的objectid到twitter的snowflake，目前国内的几个互联网大厂也开始重视到了分布式系统中数据id，甚至一些大厂已经公开了它们关于id生成器的设计和实现。我们当然也注意到了分布式系统中id的重要性，并且在系统开始开发的时候就设计并且实现了一个id生成器，我们称之为：Chaos。Chaos目前已经在我们内部运行了2年之久，在这2年内故障率为惊人0，可靠性达到不可思议的100%。那么我们为什么要设计Chaos呢？Chaos又和别人家的id生成器有什么不同呢？这些问题，首先得从我们以往的经验开始。&lt;/p&gt;

&lt;h2 id=&quot;过往使用id的经验&quot;&gt;过往使用id的经验&lt;/h2&gt;

&lt;h3 id=&quot;int自增&quot;&gt;int自增&lt;/h3&gt;
&lt;p&gt;通常情况下，说到id，我们第一个想到的就是int自增类型。它被各种“最佳实践”、教科书、论文…所推荐。然而，在现在这个信息爆炸的年代，自增id已经老态龙钟，显露颓势了！&lt;br /&gt;
从学校到社会；从教科书到实践；从办公自动化到erp再到现在的互联网和大数据，不管什么时候，不管什么年代，我们接受到对于数据id的设计原则都来源于《数据库原理》这本书：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;唯一标识，确保不重复；&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;确保主键是无意义的；&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;首推采用int作为主键值；&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;减少主键的变动；&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;在单机、单数据库、dal+proc的年代，这些原则是可取的、现实的最佳实践。依照这些原则我们也一直用的很好。直到有一天，数据猛增，单表破千万甚至上亿，需要考虑拆分表甚至拆分库的时候，问题来了：&lt;br /&gt;
&lt;strong&gt;每个表的主键都是自增，数据库扩容怎么玩？&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;int-mdb.png&quot; alt=&quot;int自增多表示意图&quot; /&gt;&lt;/p&gt;

&lt;p&gt;进过一番寻找，在数据库的int自增中找到一个属性：步长。可以根据数据表的个数来设置步长，这样就不会重复了。比如将一个单表分成3个表，那么就把步长设置为3，3个表的id就会按照如下的方式来增长：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;table-1 : 0,3,6,9,12…&lt;/li&gt;
  &lt;li&gt;table-2 : 1,4,7,10,13…&lt;/li&gt;
  &lt;li&gt;table-3 : 2,5,8,11,14…&lt;br /&gt;
这种办法确实解决了id可能重复的问题，但是同时带来了一个很让人抓狂的问题：&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;每次扩容，都需要根据数据库表的数量来定义步长，一旦有疏忽，整个数据将会受到灾难性的破坏，最严重的情况下，数据根本无法重新平衡&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;所以int自增只能在单表的情况下才有最好的表现，一旦数据超过单表的最大限度，扩容是一件很麻烦的事情。&lt;/p&gt;

&lt;h3 id=&quot;string类型&quot;&gt;String类型&lt;/h3&gt;
&lt;p&gt;因为int自增id相当难扩容，我们想起了string。其实当时不是没有想过用int来自己组合一个id，但是因为那个年代普遍还是x32机器，x64刚刚开始起来，很多系统还没升级到x64，所以对于当时的系统来说，int的值太小了。一个id需要包含很多的信息，特别对于分布式系统来说，&lt;strong&gt;包含业务信息简直就是一定需要的&lt;/strong&gt;，（PS：这里不得不说一下：我们的教材很多时候不能与时俱进。各位谨记：理论是理论，实践是实践。）所以我们第一想到了GUID（UUID）。&lt;/p&gt;

&lt;h4 id=&quot;guiduuid&quot;&gt;GUID/UUID&lt;/h4&gt;

&lt;p&gt;GUID作为id确实规避了int自增的一些问题，对于GUID来说：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;肯定是唯一的，几乎不太可能碰见碰撞的问题；&lt;/li&gt;
  &lt;li&gt;也没有业务的意义，都是根据统一的规则（并非业务规则）生成；&lt;br /&gt;
看上去倒是一个很好的id解决办法，确实也有很多公司在使用它，但是GUID也存在几个问题：&lt;/li&gt;
  &lt;li&gt;表的切分貌似只能有一种方法来确定：hash（guid）% table-count；&lt;/li&gt;
  &lt;li&gt;无法排序，对表的主键并不友好；&lt;/li&gt;
  &lt;li&gt;因为无业务意义，所以人类的识别度不高；&lt;br /&gt;
那么这些问题怎么解决呢？既然GUID的字符串都可以，那我们把字符串变成自定义的不就妥了嘛？&lt;/li&gt;
&lt;/ol&gt;

&lt;h4 id=&quot;自定义string&quot;&gt;自定义String&lt;/h4&gt;
&lt;p&gt;自定义String是我们当时的解决方案。相比于GUID，自定义String有太多的优势：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;自定义的string可以塞入任何你想要的信息，可定制性很强；&lt;/li&gt;
  &lt;li&gt;轻松的实现分库分表运算，并且不仅仅限制在hash算法；&lt;/li&gt;
  &lt;li&gt;人类的识别度很高，可以用字符串明确的标识；&lt;/li&gt;
  &lt;li&gt;可以使用本地生成，根本无延时；&lt;br /&gt;
虽然string的id优势明显，也解决掉了切分方法和识别度的重要问题，但最重要的几个缺点还存在：&lt;/li&gt;
  &lt;li&gt;string太长，且对于的运算过慢；&lt;/li&gt;
  &lt;li&gt;对排序和索引支持很不友好，对主键的索引块是破坏性的；&lt;br /&gt;
就这些缺点来说，在当时还是可以接受。但是在现在这个时代，已经不可能了，那我们到底要什么样的id呢？&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;strong&gt;重要但很少考虑的问题&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这里主要说一下id对于数据库索引的支持不友好问题！众所周知，我们目前使用的数据库，不管是sqlserver，oracle还是mysql，数据库的索引几乎都是清一色的btree或者是其衍生版。&lt;br /&gt;
在数据库的主键中，当插入数据的时候（假设我们从来不会更新主键信息，一般也确实不会更新），db会主动维护一个btree结构，这个结构最终会序列化磁盘上，在磁盘上，索引的格式我们简化如图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;string-pk-1.png&quot; alt=&quot;btree磁盘结构&quot; /&gt;&lt;/p&gt;

&lt;p&gt;每一小块表示一个id对应的信息，每一大块表示磁盘的文件块大小，每个索引最后都会被像这样子连接起来。如图看起来好像没问题，那么问题来了，如果我要插入一个id=15的值呢？结果就会像下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;string-pk-2.png&quot; alt=&quot;btree插入磁盘结构&quot; /&gt;&lt;/p&gt;

&lt;p&gt;因为前后的索引磁盘块都已经占满（或者是到一个阀值），数据库就会主动将原来的索引链断开，插入新数据，再连接上彼此的上下级索引。这种操作相比依次的插入，会带来更多的磁盘io。&lt;br /&gt;
然而，如果使用string类型的id（特别是guid），因为没办法确定顺序，所以拆开索引-插入索引的操作将会经常发生，性能当然会有问题了。&lt;/p&gt;

&lt;h2 id=&quot;初涉id&quot;&gt;初涉id&lt;/h2&gt;

&lt;h3 id=&quot;基本分析&quot;&gt;基本分析&lt;/h3&gt;

&lt;p&gt;在业务系统中，很多地方都会用到id，主要的地方有几种：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;书、卷、章节等id；&lt;/li&gt;
  &lt;li&gt;分布式系统中，系统调用的错误码；&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;对于id的几个需求：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;最基本：作为数据库记录的主键；&lt;/li&gt;
  &lt;li&gt;加强型：被索引，对索引友好；&lt;/li&gt;
  &lt;li&gt;附带价值：作为分库分表的依据；&lt;/li&gt;
  &lt;li&gt;扩展功能：对象唯一标识，比如sessionid、批号、错误号；&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;具体的分析一下在业务规则下的id，它必须具有以下的一些特性：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;分库分表：因为分库分表的方式各种各样，并且会随着业务的变化而变化，所以对于id来说，它必须要包括时间戳、随机数、类型位、数据库标识等几种最基本的属性，以供各种分库分表的方法使用；&lt;/li&gt;
  &lt;li&gt;高可用性：机器位，这是为了分布式系统中id必须唯一而设置的；&lt;/li&gt;
  &lt;li&gt;数据可读性必须强；&lt;/li&gt;
  &lt;li&gt;递增还是随机？每秒递增？？累加递增？？&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;基本诉求&quot;&gt;基本诉求&lt;/h3&gt;
&lt;p&gt;必须要具备以下一些功能或者说特性：&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;唯一，必须唯一；&lt;/li&gt;
  &lt;li&gt;短，尽可能的短；&lt;/li&gt;
  &lt;li&gt;生成速度足够快；&lt;/li&gt;
  &lt;li&gt;运算足够简单，快速；&lt;/li&gt;
  &lt;li&gt;附带实体业务信息，比如时间、类型等；&lt;/li&gt;
  &lt;li&gt;部分信息可以自定义，比如路由信息；&lt;/li&gt;
  &lt;li&gt;不仅机器能识别，人类也可以识别；&lt;/li&gt;
  &lt;li&gt;对索引友好；&lt;/li&gt;
  &lt;li&gt;根据业务规则，能自定义排序等业务规则；&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;基本方向&quot;&gt;基本方向&lt;/h3&gt;
&lt;ol&gt;
  &lt;li&gt;必须足够短，最好是uint32，最长uint64；&lt;/li&gt;
  &lt;li&gt;必须系统原生支持，不需要扩展类型；&lt;/li&gt;
  &lt;li&gt;比较运算足够快；&lt;/li&gt;
  &lt;li&gt;必须递增，可排序并对索引友好；&lt;/li&gt;
  &lt;li&gt;id必须带业务性质，符合望文生义原则，通过id可以知道这个数据存在的数据库、表等信息，如果是错误号，必须能知道所发生的服务器；&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;id的水还是很深的&quot;&gt;id的水还是很深的&lt;/h2&gt;

&lt;h3 id=&quot;snowflake&quot;&gt;snowflake&lt;/h3&gt;

&lt;p&gt;考虑实现id，twitter的snowflake算法是一个无法回避的问题，分析snowflake的算法，它有几个重要的特点：&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;首先：snowflake由时间戳-机器位-随机数组成，分别是41位、10位、12位&lt;/li&gt;
  &lt;li&gt;其次：snowflake选择了使用uint64类型，所以在这个算法下，最大值就是0XFFFF FFFF FFFF FFFF；&lt;br /&gt;
来看一个例子，比如有一个数：&lt;br /&gt;
&lt;strong&gt;9223  3720 3257 7650 688&lt;/strong&gt;  &lt;br /&gt;
这个数代表了什么意思呢？首先得分析它的二进制，这个数的二进制是：&lt;br /&gt;
&lt;strong&gt;0111 1111 1111 1111 1111 1111 1111 1111 0000 0001 0001 0000 0100 0000 0000 0000&lt;/strong&gt; &lt;br /&gt;
然后通过snowflake的组成机制，算得这个数的真实要代表的数：&lt;br /&gt;
&lt;strong&gt;2199023251472-264-0&lt;/strong&gt;&lt;br /&gt;
不得不说，这种二进制移位的方法对于机器来说非常的简单、运算也更快，但是对于程序员来说，简直就是天书。所以snowflake算法我们并不满意，对于snowflake不满意的并不仅仅“把技术人员不当人”这一项，还有2个业务的问题snowflake也无法解决：
    &lt;ol&gt;
      &lt;li&gt;没有类型信息；&lt;/li&gt;
      &lt;li&gt;分库分表没有数据库定位信息；&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;chaos的设计&quot;&gt;Chaos的设计&lt;/h3&gt;

&lt;h4 id=&quot;场景&quot;&gt;场景&lt;/h4&gt;

&lt;p&gt;Chaos首先确定的就是放弃二进制而选用十进制。使用十进制最主要就是id的特殊场景。一般来说，id对于一个对象就像是身份证对于你一样，如果不去办银行卡、订酒店、订机票…，在日常生活中，身份证永远都是安静的躺在你的钱包里面；同理映射到技术人员的开发日常，当程序员要注意一个id的时候，就是当且仅当系统出问题了，需要排查。如果排错的时候，使用的还是snowflake这种二进制移位方法生成的id，同时领导、同事催促的电话频繁响起（往往这时候老板的电话特别多），然后第一句就是：怎么又出问题了？一身汗的同时你能第一时间确定这个id到底来自哪里？什么类型？用处是什么？… 这种情况下，你除了想打人已经没有别的想法了！&lt;/p&gt;

&lt;h4 id=&quot;组成&quot;&gt;组成&lt;/h4&gt;

&lt;p&gt;Chaos也和snowflake一样，选用了uint64类型，但是因为Chaos是10进制，所以对于Chaos，uint64的最大值是&lt;br /&gt;
&lt;strong&gt;1844 6744 0737 0955 1656&lt;/strong&gt;&lt;br /&gt;
那么Chaos最大值只能是&lt;br /&gt;
&lt;strong&gt;9999 9999 9999 9999 999&lt;/strong&gt;&lt;br /&gt;
直观来看，就是Chaos的id少了一位。这又是为什么呢？接着来看：&lt;br /&gt;
和snowklake一样，Chaos也是选择了使用指定位数来确定业务信息，不同的是Chaos的位数是10进制位数。我们的算法：&lt;br /&gt;
&lt;strong&gt;时间戳-随机数位-类型位-机器位-数据库标识&lt;/strong&gt; ，10位 + 4位 + 2位 + 1位 + 2位&lt;br /&gt;
这里就可以解释为什么Chaos的id会少一位了！Chaos的id最前面的几位是时间戳，而u64的最大值最前面的数值是1，如果坚持原定的位数Chaos生成id的时间戳就可能会溢出。所以Chaos的id干脆少一位，这样不管最前面的数值多大，就算是9，因为少了一位，所以肯定不会溢出。&lt;/p&gt;

&lt;h4 id=&quot;序列号&quot;&gt;序列号&lt;/h4&gt;

&lt;p&gt;在Chaos中，选择了序列号而并不是随机数来解决数的唯一性问题。之所以不选择随机数，是因为随机数在5位数的情况下，万分之一的碰撞概率对于10k qps的压力来说还是挺大的，所以随机数在Chaos的需求下并不是一个很好的方案。&lt;br /&gt;
序列号就是一个计数器，从0-9999计数，9999后直接归0.这种累加方式在Chaos中被称之为累加递增。还有一些业务确实需要每次都是从0开始的数，这种情况下的递增在Chaos中被称之为每秒递增。&lt;br /&gt;
从方案上来说，结合业务，累加递增可以让id更加均衡，id可以被更加均匀的分配到每个库和每个表中，所以它更适合用来做分库分表；而每秒递增更适合于重排序的情况。&lt;/p&gt;

&lt;h4 id=&quot;递增性分析&quot;&gt;递增性分析&lt;/h4&gt;
&lt;p&gt;上面讲到的序列号递增方法，因为序列号是id的一部分，所以序列号的递增性其实决定了整个id的递增性。也就是说，序列号决定了id的递增性。例如：&lt;br /&gt;
&lt;strong&gt;累加递增：秒内进位&lt;/strong&gt;&lt;br /&gt;
429497-9998-01-1-00&lt;br /&gt;
429498-9999-01-1-00&lt;br /&gt;
429498-0000-01-1-00&lt;br /&gt;
&lt;strong&gt;每秒递增：设计成每秒10k个，超过不会放出id&lt;/strong&gt; &lt;br /&gt;
429496-0000-01-1-00&lt;br /&gt;
429497-0000-01-1-00&lt;br /&gt;
429498-0000-01-1-00&lt;/p&gt;

&lt;p&gt;所以查看这上面的id，我们可以总结如下：
&lt;img src=&quot;id-3.png&quot; alt=&quot;id趋势&quot; /&gt;
累加递增：长时间（2s内）内保证单调递增，短时间（1s内）内不保证单调递增&lt;br /&gt;
每秒递增：它肯定是递增的，因为每秒都会从0开始，单位时间内都是单调递增&lt;/p&gt;

&lt;h4 id=&quot;id的使用&quot;&gt;id的使用&lt;/h4&gt;
&lt;p&gt;对于分库分表的使用，chaos生成的id使用示意如下：&lt;br /&gt;
&lt;img src=&quot;id-5.png&quot; alt=&quot;id router&quot; /&gt;&lt;/p&gt;

&lt;p&gt;对于错误号的使用，chaos生成的id使用示意如下：
&lt;img src=&quot;id-6.png&quot; alt=&quot;id error&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;服务器设计&quot;&gt;服务器设计&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;id-4.png&quot; alt=&quot;id server&quot; /&gt;
chaos采用了水平的分布式设计。服务器都是无状态的，也是去中心化的。这样的设计可以更好的来适应后面压力增大后对于服务器的需求。目前Chaos只支持一个集群中最多有10台服务器提供id服务。是不是感觉少了一些，其实够用了。按照chaos的设计需求，10台chaos每秒可以生成：10(台) * 100(类型数) * 10000(每秒最大数量）= 1000 0000个id。&lt;/p&gt;

&lt;h2 id=&quot;总结&quot;&gt;总结&lt;/h2&gt;
&lt;p&gt;id，其实就是一个19位的数字。对于id来说，技术含量不在于纯技术，而在于对系统的架构控制，更是在于提升id对业务系统的最大化作用和控制。&lt;/p&gt;

</description>
        <pubDate>Wed, 31 May 2017 00:00:00 +0800</pubDate>
        <link>http://www.94geek.com/blog/2017/idcreator-chaos/</link>
        <guid isPermaLink="true">http://www.94geek.com/blog/2017/idcreator-chaos/</guid>
      </item>
    
  </channel>
</rss>
